{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec7fb97b",
   "metadata": {},
   "source": [
    "# **--- CNN MODEL ---** #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d3b0f5",
   "metadata": {},
   "source": [
    "## **I. Libraries import** ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a101012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import mlflow\n",
    "import tempfile\n",
    "import boto3\n",
    "\n",
    "# Torch ------------------\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "#import torchvision.transforms.v2 as v2\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models,datasets\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Metrics ------------------\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Visualization ---------\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1abf04",
   "metadata": {},
   "source": [
    "We select the appropriate torch device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b28c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "#device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():  # Apple M1/M2/M3\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a30955",
   "metadata": {},
   "source": [
    "## **II. Images import and processing** ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ffdbe",
   "metadata": {},
   "source": [
    "### Load dataset ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9148d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inrae\n",
      "../data-inrae\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME=os.getenv(\"DATASET_NAME\",\"inrae\")\n",
    "DATASET_DIR=Path(f\"../data-{DATASET_NAME}\")\n",
    "print(DATASET_NAME)\n",
    "print(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "694588e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sain: 350 images\n",
      "Nothing to delete\n"
     ]
    }
   ],
   "source": [
    "#  We randomly reduce the amount of images in the \"healthy\" class that is too represented : \n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "TARGET_NB = 350\n",
    "sain = Path(f\"{DATASET_DIR}/raw_data_inrae/sain\")\n",
    "\n",
    "images = list(sain.glob(\"*\"))\n",
    "n_images = len(images)\n",
    "\n",
    "print(f\"{sain.name}: {n_images} images\")\n",
    "\n",
    "if n_images <= TARGET_NB:\n",
    "    print(\"Nothing to delete\")\n",
    "else:\n",
    "    images_to_delete = random.sample(\n",
    "        images, n_images - TARGET_NB\n",
    "    )\n",
    "\n",
    "    for img in images_to_delete:\n",
    "        img.unlink()\n",
    "\n",
    "    print(f\"{len(images_to_delete)} images deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3ca296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Clean the target folder\n",
    "organized_data = Path(f\"../{DATASET_DIR}/organized_data_inrae\")\n",
    "\n",
    "if organized_data.exists():\n",
    "    shutil.rmtree(organized_data)\n",
    "\n",
    "\n",
    "organized_data.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --------------   Divide the dataset into Train, Val and Test -------------------\n",
    "random.seed(42)\n",
    "\n",
    "raw_data_path = Path(f\"{DATASET_DIR}/raw_data_inrae\")\n",
    "organized_data = Path(f\"{DATASET_DIR}/organized_data_inrae\")\n",
    "\n",
    "SPLITS = {\n",
    "    \"train\": 0.7,\n",
    "    \"val\": 0.15,\n",
    "    \"test\": 0.15\n",
    "}\n",
    "\n",
    "# Create folder structure\n",
    "for split in SPLITS:\n",
    "    for class_dir in raw_data_path.iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            (organized_data / split / class_dir.name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Split images\n",
    "for class_dir in raw_data_path.iterdir():\n",
    "    if not class_dir.is_dir():\n",
    "        continue\n",
    "\n",
    "    images = [\n",
    "        img for img in class_dir.iterdir()\n",
    "        if img.suffix.lower() in {\".jpg\", \".jpeg\", \".png\"}\n",
    "    ]\n",
    "\n",
    "    random.shuffle(images)\n",
    "\n",
    "    n_total = len(images)\n",
    "    n_train = int(n_total * SPLITS[\"train\"])\n",
    "    n_val = int(n_total * SPLITS[\"val\"])\n",
    "\n",
    "    train_imgs = images[:n_train]\n",
    "    val_imgs = images[n_train:n_train + n_val]\n",
    "    test_imgs = images[n_train + n_val:]\n",
    "\n",
    "    for img in train_imgs:\n",
    "        shutil.copy(img, organized_data / \"train\" / class_dir.name / img.name)\n",
    "\n",
    "    for img in val_imgs:\n",
    "        shutil.copy(img, organized_data / \"val\" / class_dir.name / img.name)\n",
    "\n",
    "    for img in test_imgs:\n",
    "        shutil.copy(img, organized_data / \"test\" / class_dir.name / img.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a13b510f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN\n",
      "  colomerus_vitis      : 119\n",
      "  erysiphe_necator     : 91\n",
      "  guignardia_bidwellii : 169\n",
      "  phaeomoniella_chlamydospora : 82\n",
      "  plasmopara_viticola  : 345\n",
      "  sain                 : 318\n",
      "  elsinoe_ampelina     : 262\n",
      "  TOTAL train          : 1386\n",
      "\n",
      "VAL\n",
      "  colomerus_vitis      : 34\n",
      "  erysiphe_necator     : 27\n",
      "  guignardia_bidwellii : 51\n",
      "  phaeomoniella_chlamydospora : 25\n",
      "  plasmopara_viticola  : 106\n",
      "  sain                 : 98\n",
      "  elsinoe_ampelina     : 81\n",
      "  TOTAL val            : 422\n",
      "\n",
      "TEST\n",
      "  colomerus_vitis      : 38\n",
      "  erysiphe_necator     : 31\n",
      "  guignardia_bidwellii : 54\n",
      "  phaeomoniella_chlamydospora : 25\n",
      "  plasmopara_viticola  : 105\n",
      "  sain                 : 98\n",
      "  elsinoe_ampelina     : 83\n",
      "  TOTAL test           : 434\n"
     ]
    }
   ],
   "source": [
    "# We check the size of the new folders\n",
    "\n",
    "ROOT_DIR = Path(f\"{DATASET_DIR}/organized_data_inrae\")\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    print(f\"\\n{split.upper()}\")\n",
    "    total = 0\n",
    "\n",
    "    for class_dir in (ROOT_DIR / split).iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            n_files = len(list(class_dir.glob(\"*\")))\n",
    "            total += n_files\n",
    "            print(f\"  {class_dir.name:<20} : {n_files}\")\n",
    "\n",
    "    print(f\"  TOTAL {split:<14} : {total}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e4a4c",
   "metadata": {},
   "source": [
    "### Pipeline for data transformation ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5c1385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations of the train set with data augmentation\n",
    "\n",
    "transform_train= transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=(-45,+45)),\n",
    "    transforms.ToTensor(),        \n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225])  # Normalization with the values for the pre-trained Resnet model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29580af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the path to the training dataset\n",
    "data_train = Path(f\"{DATASET_DIR}/organized_data_inrae/train\")\n",
    "\n",
    "# Load dataset with ImageFolder\n",
    "train_dataset = ImageFolder(root=data_train, transform=transform_train)\n",
    "\n",
    "# Get class names\n",
    "class_names = train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006dc0c",
   "metadata": {},
   "source": [
    "We build a dictionnary with translations and we send a JSON file to our S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba2c9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"colomerus_vitis\": \"Erinose\",\n",
      "    \"elsinoe_ampelina\": \"Anthracnose\",\n",
      "    \"erysiphe_necator\": \"Oïdium\",\n",
      "    \"guignardia_bidwellii\": \"Pourriture_noire\",\n",
      "    \"phaeomoniella_chlamydospora\": \"Esca\",\n",
      "    \"plasmopara_viticola\": \"Mildiou\",\n",
      "    \"sain\": \"Pas de maladie\"\n",
      "}\n",
      "/tmp/tmp1szw4gca/disease-inrae.json\n",
      "Disease dictionnary uploaded to : s3://aws-s3-mlflow/vitiscan-data/disease-inrae.json\n"
     ]
    }
   ],
   "source": [
    "translations = [\n",
    "    \"Erinose\",\n",
    "    \"Anthracnose\",\n",
    "    \"Oïdium\",\n",
    "    \"Pourriture_noire\",\n",
    "    \"Esca\",\n",
    "    \"Mildiou\",\n",
    "    \"Pas de maladie\"\n",
    "]\n",
    "\n",
    "DISEASES = {}\n",
    "for c,t in zip(class_names, translations):\n",
    "    DISEASES[c] = t\n",
    "\n",
    "print(json.dumps(DISEASES, indent=4, ensure_ascii=False))\n",
    "\n",
    "S3_BUCKET_NAME= os.getenv('S3_BUCKET_NAME', \"aws-s3-mlflow\")\n",
    "\n",
    "# Temporary file for saving JSON file\n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        s3 = boto3.client('s3')\n",
    "        path = Path(tmp_dir, f\"disease-{DATASET_NAME}.json\")\n",
    "        print(str(path))\n",
    "        with path.open('w') as f:\n",
    "            json.dump(DISEASES, f)\n",
    "        dest_file_name = f'vitiscan-data/disease-{DATASET_NAME}.json'\n",
    "        s3.upload_file(Bucket=S3_BUCKET_NAME, Filename=str(path), Key=dest_file_name)\n",
    "        s3.close()\n",
    "        print(f\"Disease dictionnary uploaded to : s3://{S3_BUCKET_NAME}/{dest_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8841b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Transformation pipeline without data augmentation for the validation and the test set\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Redimensionnement à 224x224\n",
    "    transforms.ToTensor(),          # Conversion en tenseur\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225])  # Normalisation avec les valeurs du modèle pré-entraîné Resnet\n",
    "])\n",
    "\n",
    "# Create the path to the training dataset\n",
    "data_val = Path(f\"{DATASET_DIR}/organized_data_inrae/val\")\n",
    "data_test = Path(f\"{DATASET_DIR}/organized_data_inrae/test\")\n",
    "\n",
    "# Load dataset with ImageFolder\n",
    "val_dataset = ImageFolder(root=data_val,transform=transform_test)\n",
    "test_dataset = ImageFolder(root=data_test, transform=transform_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "174b6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b7e983",
   "metadata": {},
   "source": [
    "## **Fine tuning (Resnet18)** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a677dfea",
   "metadata": {},
   "source": [
    "### Preparing the MLFlow tracking ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9da841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tracking URI to your Hugging Face application\n",
    "MLFLOW_URI=os.getenv('MLFLOW_URI',\"https://gviel-mlflow37.hf.space/\")\n",
    "mlflow.set_tracking_uri(os.environ[\"MLFLOW_URI\"])\n",
    "\n",
    "# Set experiment's info\n",
    "EXPERIMENT_NAME= os.getenv('EXPERIMENT_NAME',\"Vitiscan_CNN_MLFlow\")+\"_FINE_TUNING\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# Get our experiment info\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d57000",
   "metadata": {},
   "source": [
    "### Importing a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb1666c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try to instanciate a model : Resnet34\n",
      "model type = ResNet34\n",
      "Model device: cuda:0\n",
      "device = cuda\n"
     ]
    }
   ],
   "source": [
    "# instanciate model\n",
    "MODEL_NAME = os.getenv('MODEL_NAME', 'resnet18').lower().capitalize()\n",
    "print(f\"Try to instanciate a model : {MODEL_NAME}\")\n",
    "model = eval(f\"models.{MODEL_NAME.lower()}(weights='DEFAULT')\") # pour version >=0.13.0\n",
    "\n",
    "# check model\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "if num_params < 15_000_000:\n",
    "    depth = 18\n",
    "else:\n",
    "    depth = 34\n",
    "print(f\"model type = {type(model).__name__}{depth}\")\n",
    "\n",
    "# switch the model to the best available DEVICE\n",
    "model = model.to(device)\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(\"device =\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c762ac3b",
   "metadata": {},
   "source": [
    "### Replace classification layer to adapt the model to our features ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ff3244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes=len(train_dataset.classes)\n",
    "model.fc = nn.Linear(model.fc.in_features, nb_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb65f752",
   "metadata": {},
   "source": [
    "### Freeze the feature extraction layers ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c5d072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We freeze the entire network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# We unfreeze only the last Resnet18 blocks of layers (called \"Layer4 for resnet18\")\n",
    "# AND we unfreeze the classifier fc\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer4\" in name or \"fc\" in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dec27343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 7]                    --\n",
       "├─Conv2d: 1-1                            [1, 64, 112, 112]         (9,408)\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 112, 112]         (128)\n",
       "├─ReLU: 1-3                              [1, 64, 112, 112]         --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 56, 56]           --\n",
       "├─Sequential: 1-5                        [1, 64, 56, 56]           --\n",
       "│    └─BasicBlock: 2-1                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 56, 56]           (36,864)\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 56, 56]           (128)\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 56, 56]           (36,864)\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 56, 56]           (128)\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 56, 56]           --\n",
       "│    └─BasicBlock: 2-2                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-7                  [1, 64, 56, 56]           (36,864)\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 64, 56, 56]           (128)\n",
       "│    │    └─ReLU: 3-9                    [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-10                 [1, 64, 56, 56]           (36,864)\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 56, 56]           (128)\n",
       "│    │    └─ReLU: 3-12                   [1, 64, 56, 56]           --\n",
       "│    └─BasicBlock: 2-3                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-13                 [1, 64, 56, 56]           (36,864)\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 64, 56, 56]           (128)\n",
       "│    │    └─ReLU: 3-15                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-16                 [1, 64, 56, 56]           (36,864)\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 64, 56, 56]           (128)\n",
       "│    │    └─ReLU: 3-18                   [1, 64, 56, 56]           --\n",
       "├─Sequential: 1-6                        [1, 128, 28, 28]          --\n",
       "│    └─BasicBlock: 2-4                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-19                 [1, 128, 28, 28]          (73,728)\n",
       "│    │    └─BatchNorm2d: 3-20            [1, 128, 28, 28]          (256)\n",
       "│    │    └─ReLU: 3-21                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-22                 [1, 128, 28, 28]          (147,456)\n",
       "│    │    └─BatchNorm2d: 3-23            [1, 128, 28, 28]          (256)\n",
       "│    │    └─Sequential: 3-24             [1, 128, 28, 28]          (8,448)\n",
       "│    │    └─ReLU: 3-25                   [1, 128, 28, 28]          --\n",
       "│    └─BasicBlock: 2-5                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-26                 [1, 128, 28, 28]          (147,456)\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 128, 28, 28]          (256)\n",
       "│    │    └─ReLU: 3-28                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 128, 28, 28]          (147,456)\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 128, 28, 28]          (256)\n",
       "│    │    └─ReLU: 3-31                   [1, 128, 28, 28]          --\n",
       "│    └─BasicBlock: 2-6                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-32                 [1, 128, 28, 28]          (147,456)\n",
       "│    │    └─BatchNorm2d: 3-33            [1, 128, 28, 28]          (256)\n",
       "│    │    └─ReLU: 3-34                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-35                 [1, 128, 28, 28]          (147,456)\n",
       "│    │    └─BatchNorm2d: 3-36            [1, 128, 28, 28]          (256)\n",
       "│    │    └─ReLU: 3-37                   [1, 128, 28, 28]          --\n",
       "│    └─BasicBlock: 2-7                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-38                 [1, 128, 28, 28]          (147,456)\n",
       "│    │    └─BatchNorm2d: 3-39            [1, 128, 28, 28]          (256)\n",
       "│    │    └─ReLU: 3-40                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-41                 [1, 128, 28, 28]          (147,456)\n",
       "│    │    └─BatchNorm2d: 3-42            [1, 128, 28, 28]          (256)\n",
       "│    │    └─ReLU: 3-43                   [1, 128, 28, 28]          --\n",
       "├─Sequential: 1-7                        [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-8                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-44                 [1, 256, 14, 14]          (294,912)\n",
       "│    │    └─BatchNorm2d: 3-45            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-46                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-47                 [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-48            [1, 256, 14, 14]          (512)\n",
       "│    │    └─Sequential: 3-49             [1, 256, 14, 14]          (33,280)\n",
       "│    │    └─ReLU: 3-50                   [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-9                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-51                 [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-52            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-53                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-54                 [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-55            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-56                   [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-10                  [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-57                 [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-58            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-59                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-60                 [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-61            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-62                   [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-11                  [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-63                 [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-64            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-65                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-66                 [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-67            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-68                   [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-12                  [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-69                 [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-70            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-71                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-72                 [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-73            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-74                   [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-13                  [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-75                 [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-76            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-77                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-78                 [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-79            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-80                   [1, 256, 14, 14]          --\n",
       "├─Sequential: 1-8                        [1, 512, 7, 7]            --\n",
       "│    └─BasicBlock: 2-14                  [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-81                 [1, 512, 7, 7]            1,179,648\n",
       "│    │    └─BatchNorm2d: 3-82            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-83                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-84                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-85            [1, 512, 7, 7]            1,024\n",
       "│    │    └─Sequential: 3-86             [1, 512, 7, 7]            132,096\n",
       "│    │    └─ReLU: 3-87                   [1, 512, 7, 7]            --\n",
       "│    └─BasicBlock: 2-15                  [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-88                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-89            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-90                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-91                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-92            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-93                   [1, 512, 7, 7]            --\n",
       "│    └─BasicBlock: 2-16                  [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-94                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-95            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-96                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-97                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-98            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-99                   [1, 512, 7, 7]            --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --\n",
       "├─Linear: 1-10                           [1, 7]                    3,591\n",
       "==========================================================================================\n",
       "Total params: 21,288,263\n",
       "Trainable params: 13,117,959\n",
       "Non-trainable params: 8,170,304\n",
       "Total mult-adds (Units.GIGABYTES): 3.66\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 59.81\n",
       "Params size (MB): 85.15\n",
       "Estimated Total Size (MB): 145.57\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print model summary\n",
    "summary(model, input_size=(1, 3, 224, 224))  # (batch_size, input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827dc7ed",
   "metadata": {},
   "source": [
    "### Defining the cost function and optimizer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c085ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device) \n",
    "\n",
    "learning_rate=0.0001\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=0.0001 # Ridge regulation to avoid overfitting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94632022",
   "metadata": {},
   "source": [
    "### Train the model ###\n",
    "\n",
    "We check an output on the fist batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "222aa307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.4174e-01, -1.0914e-01,  7.8088e-01,  2.6537e-01, -1.0031e+00,\n",
       "         -4.8731e-01,  7.8230e-01],\n",
       "        [-3.8544e-01, -1.2315e+00,  9.8986e-01, -6.5004e-01,  5.6175e-02,\n",
       "         -5.9343e-01,  1.3975e+00],\n",
       "        [-8.1700e-03, -7.4442e-01,  7.2297e-01, -6.3487e-01,  3.9371e-01,\n",
       "          2.1825e-01,  7.4311e-01],\n",
       "        [ 4.7334e-01, -7.8436e-01,  1.1371e+00, -1.0376e+00, -9.4818e-01,\n",
       "          6.4537e-01,  5.2991e-02],\n",
       "        [-9.4738e-02, -2.1556e-01,  1.0670e+00, -9.6417e-01, -1.2588e-01,\n",
       "          9.8969e-01,  4.2063e-01],\n",
       "        [ 8.9900e-02, -3.9654e-01,  7.1174e-01, -5.2122e-01, -1.0204e-01,\n",
       "          1.6413e-01,  1.1436e+00],\n",
       "        [-1.9413e-01, -4.8028e-01,  1.3035e+00, -7.5769e-01, -2.8702e-01,\n",
       "          1.9341e-01, -1.1908e-02],\n",
       "        [ 1.1445e-01, -5.9714e-01,  7.3823e-01,  2.5782e-02, -4.4456e-01,\n",
       "          2.3987e-01,  4.7326e-01],\n",
       "        [ 3.7134e-01, -1.1807e+00,  1.1909e+00, -4.9011e-01, -2.3649e-01,\n",
       "         -2.8221e-01,  5.8639e-01],\n",
       "        [-1.3429e-01, -6.3652e-01,  1.1488e+00, -3.6658e-01, -1.1388e-01,\n",
       "         -5.8463e-01,  3.9280e-01],\n",
       "        [ 1.8996e-01, -1.0849e+00,  1.1612e+00, -6.4216e-01,  1.8141e-01,\n",
       "          5.7385e-01,  1.3481e+00],\n",
       "        [-8.3752e-01, -2.6835e-01,  5.2800e-01, -4.1655e-01,  3.4206e-03,\n",
       "          1.0271e+00,  9.0895e-01],\n",
       "        [-2.4288e-01, -4.0363e-01,  9.8815e-01, -1.0514e+00,  9.1965e-02,\n",
       "         -3.5363e-01,  1.5366e+00],\n",
       "        [ 2.6362e-01, -9.5157e-01,  3.6714e-02, -1.1607e+00, -3.3793e-01,\n",
       "          5.9929e-01,  1.1000e+00],\n",
       "        [-3.7318e-01, -2.4352e-01,  1.2068e+00, -5.4246e-01, -1.4085e-02,\n",
       "          6.6792e-01,  8.4634e-01],\n",
       "        [ 1.1246e+00,  3.2026e-01,  5.3298e-01, -1.8836e+00,  3.1085e-01,\n",
       "         -4.2092e-02,  8.5308e-01],\n",
       "        [ 1.5114e-01, -7.1256e-01,  5.6476e-01, -8.2605e-01,  3.5797e-01,\n",
       "          7.8263e-01,  5.0027e-01],\n",
       "        [ 1.5856e-03, -5.9095e-01,  5.4850e-01, -5.7258e-01,  6.7150e-01,\n",
       "          8.0125e-01,  8.4422e-01],\n",
       "        [-4.3101e-01, -8.6154e-01,  1.0613e+00, -7.4710e-01, -1.0639e-01,\n",
       "         -2.3550e-01,  1.0008e+00],\n",
       "        [-6.2046e-01, -5.2153e-01,  1.1866e+00, -1.8524e-01, -3.6845e-01,\n",
       "          7.1005e-01,  4.1410e-01],\n",
       "        [-6.2048e-01,  4.9784e-02,  8.7171e-01, -1.1129e+00, -6.3491e-02,\n",
       "         -2.8738e-02,  1.0342e+00],\n",
       "        [-3.1392e-01,  1.3149e-01,  1.0860e+00, -4.6570e-01, -4.1133e-01,\n",
       "          3.2615e-01,  6.0923e-01],\n",
       "        [ 3.6952e-01, -7.0767e-01,  1.0781e+00, -1.4272e-01,  6.6017e-01,\n",
       "         -1.8604e-01,  1.3598e+00],\n",
       "        [ 3.9453e-01, -9.6600e-01,  4.8893e-01, -2.9970e-01, -7.1270e-01,\n",
       "          1.2675e+00,  2.3300e-01],\n",
       "        [-9.4153e-01, -6.5399e-01,  1.5265e+00, -3.6838e-01,  1.6665e-01,\n",
       "          1.2396e+00,  5.7699e-01],\n",
       "        [-5.0310e-01, -1.1460e-01,  1.0311e+00, -2.4100e-01, -4.9485e-01,\n",
       "          6.6793e-01,  1.3389e+00],\n",
       "        [ 2.1906e-01, -8.7868e-01,  8.2307e-01, -1.6737e+00,  2.5800e-01,\n",
       "          3.2936e-01,  8.6148e-01],\n",
       "        [-2.4763e-01, -6.6646e-01,  9.4219e-01, -9.9768e-01, -3.0728e-01,\n",
       "         -1.9055e-01,  1.3265e+00],\n",
       "        [-3.1677e-01, -2.2196e-01,  1.1618e+00, -8.9368e-01, -3.0584e-01,\n",
       "          8.7394e-01,  1.6011e+00],\n",
       "        [-2.5030e-03, -2.4703e-01,  1.0400e+00, -1.0488e+00, -5.6245e-01,\n",
       "          5.8495e-01,  1.2640e+00],\n",
       "        [ 2.3220e-01, -4.6260e-01,  1.4424e+00, -2.7358e-01,  2.2225e-01,\n",
       "          6.0840e-01,  1.0186e+00],\n",
       "        [ 1.3382e-01, -1.2131e+00,  4.4970e-01, -1.4168e-01, -1.1193e+00,\n",
       "         -1.0333e-01,  1.0101e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model outputs\n",
    "image, label = next(iter(train_loader))\n",
    "image=image.to(device)\n",
    "model=model.to(device)\n",
    "logit= model(image) # Resnet18 output\n",
    "logit\n",
    "# Attention il faut que l'image et le modele aient le même device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6900526c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['colomerus_vitis',\n",
       " 'elsinoe_ampelina',\n",
       " 'erysiphe_necator',\n",
       " 'guignardia_bidwellii',\n",
       " 'phaeomoniella_chlamydospora',\n",
       " 'plasmopara_viticola',\n",
       " 'sain']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be446dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def evaluate_model_on_test(model, test_loader, device):\n",
    "    \n",
    "    model.eval() # Mode évaluation\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logit = model(images)\n",
    "            preds = logit.argmax(dim=1)\n",
    "            \n",
    "            # Compter l'exactitude\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            \n",
    "            # Collecter pour les métriques de Scikit-learn\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    \n",
    "    test_accuracy = correct / total\n",
    "    return test_accuracy, y_true, y_pred\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc723ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_dataset(model, data_loader:DataLoader, device:str):\n",
    "    '''\n",
    "    Evaluate a dataset with the model\n",
    "    '''\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    model.eval() # passage du modèle en mode évaluation/prédiction\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logit = model(images)\n",
    "            preds = logit.argmax(dim=1)\n",
    "\n",
    "            # Compter l'exactitude des prédictions\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy, y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb409d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_confusion_matrix(dataset_type:str, y_true:list, y_pred:list):\n",
    "    ''' Generate a Confusion Matrix and log it into MLFlow'''\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "            cm_test = confusion_matrix(y_true, y_pred)\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.xticks(ticks= range(nb_classes),labels=class_names, rotation=45, ha=\"right\")\n",
    "            plt.yticks(ticks= range(nb_classes),labels=class_names,rotation=0)\n",
    "            plt.ylabel('True class')\n",
    "            plt.xlabel('Predicted class')\n",
    "            plt.title(f'Confusion matrix - {dataset_type}')\n",
    "            plt.tight_layout()\n",
    "            path = str(Path(tmp_dir, f\"confusion_matrix_{dataset_type}.png\"))\n",
    "            plt.savefig(path, dpi=150)\n",
    "            mlflow.log_artifact(path)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b130c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_precision_recall_f1_score(dataset_type:str, y_true:list, y_pred:list):\n",
    "    '''\n",
    "        Compute various scores and log them into MLFlow\n",
    "        Return the scores in a dict\n",
    "    '''\n",
    "    results = {}\n",
    "    average_modes=['weighted', 'macro']\n",
    "    for avg_mode in average_modes:\n",
    "        results[avg_mode] = {}\n",
    "        for score in ['precision', 'recall', 'f1']:\n",
    "            # utilisation méthode eval pour exécuter les 3 méthodes *_score()\n",
    "            metric_value = eval(f'{score}_score(y_true, y_pred, average=avg_mode, zero_division=0)')\n",
    "            metric_name = f\"{dataset_type.capitalize()}_{score}_{avg_mode}\"\n",
    "            results[avg_mode][score] = metric_value\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c354fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for a PyTorch model\n",
    "def train(model, train_loader, val_loader, test_loader, criterion, optimizer, experiment, epochs=20, patience=5):\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # We start a MLflow run\n",
    "    with mlflow.start_run(experiment_id=experiment.experiment_id) as active_run:\n",
    "\n",
    "        # Logging Pytorch parameters into MLflow \n",
    "        params = {\n",
    "            \"optimizer\": type(optimizer).__name__,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"epochs\": epochs,\n",
    "            \"criterion\": type(criterion).__name__,\n",
    "            \"model_architecture\": type(model).__name__,\n",
    "            \"training_device\": str(device),\n",
    "            \"weight_decay\": optimizer.param_groups[0][\"weight_decay\"]\n",
    "        }\n",
    "        \n",
    "        mlflow.log_params(params=params)\n",
    "        mlflow.pytorch.autolog()\n",
    "\n",
    "        # Dictionary to store loss and accuracy values for each epoch\n",
    "        history = {\n",
    "            'loss': [],\n",
    "            'val_loss': [],\n",
    "            'accuracy': [],\n",
    "            'val_accuracy': []\n",
    "        }\n",
    "\n",
    "        # ------------------- TRAINING LOOP -------------------\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_loss, correct = 0, 0  \n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logit = model(images)\n",
    "                loss = criterion(logit, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                correct += (logit.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "            train_loss = total_loss / len(train_loader)\n",
    "            train_acc = correct / len(train_loader.dataset)\n",
    "\n",
    "            # ------------------- VALIDATION LOOP -------------------\n",
    "            model.eval()\n",
    "            val_loss, val_correct = 0, 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    logit = model(images)\n",
    "                    loss = criterion(logit, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    val_correct += (logit.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "            # --- Save metrics ---\n",
    "            history['loss'].append(train_loss)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['accuracy'].append(train_acc)\n",
    "            history['val_accuracy'].append(val_acc)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{epochs}], \"\n",
    "                f\"Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, \"\n",
    "                f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "            #  Logging metrics\n",
    "            #TODO voir comment ajouter le nom du Dataset dans les paramètres pour MLFLow\n",
    "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "            mlflow.log_metric(\"train_accuracy\", train_acc, step=epoch)\n",
    "            mlflow.log_metric(\"validation_loss\", val_loss, step=epoch)\n",
    "            mlflow.log_metric(\"validation_accuracy\", val_acc, step=epoch)\n",
    "\n",
    "            # --- Early stopping check ---\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = model.state_dict()  # save the best model\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                    model.load_state_dict(best_model_state)  # restore best model\n",
    "                    break\n",
    "                \n",
    "        # Final metrics logging\n",
    "        mlflow.log_metric(\"final_validation_accuracy\", final_val_acc)\n",
    "        print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "        mlflow.log_metric(\"final_train_loss\", train_loss)\n",
    "        print(f\"Final Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # ================= FINAL VALIDATION EVALUATION =================\n",
    "        print(\"\\n--- Final evaluation on the VALIDATION set ---\\n\")\n",
    "        # NB: final_val_acc already calculated before\n",
    "        final_val_acc, y_true_val, y_pred_val = evaluate_model_on_dataset(model, val_loader, device)\n",
    "        print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "\n",
    "        # --- Compute scores (Validation) ---\n",
    "        val_scores = log_precision_recall_f1_score('validation', y_true_val, y_pred_val)\n",
    "        for mode in val_scores.keys():\n",
    "            for score in val_scores[mode].keys():\n",
    "                print(f\"{score.capitalize()} Validation ({mode}): {val_scores[mode][score]:.4f}\")\n",
    "        \n",
    "        # --- Confusion Matrix (Validation) ---\n",
    "        log_confusion_matrix('VALIDATION', y_true_val, y_pred_val)\n",
    "\n",
    "        # ================= FINAL TEST EVALUATION =================\n",
    "        print(\"\\n--- Final evaluation on the TEST set ---\\n\")\n",
    "        test_acc, y_true_test, y_pred_test = evaluate_model_on_dataset(model, test_loader, device)\n",
    "\n",
    "        # --- Compute scores (test) ---\n",
    "        test_scores = log_precision_recall_f1_score('test', y_true_test, y_pred_test)\n",
    "        for mode in test_scores.keys():\n",
    "            for score in test_scores[mode].keys():\n",
    "                print(f\"{score.capitalize()} Validation ({mode}): {test_scores[mode][score]:.4f}\")\n",
    "\n",
    "        # --- Accuracy (test) ---\n",
    "        mlflow.log_metric(\"Test_accuracy\", test_acc)\n",
    "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "        # --- Confusion Matrix (Test) ---\n",
    "        log_confusion_matrix('TEST', y_true_test, y_pred_test)\n",
    "\n",
    "        # ================= MODEL LOGGING (artifacts) =================\n",
    "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "            # save a disease.json file in a temporary dir\n",
    "            path = Path(tmp_dir, f\"disease-{DATASET_NAME}.json\")\n",
    "            print(str(path))\n",
    "            with path.open('w') as f:\n",
    "                json.dump(DISEASES, f)\n",
    "            # model logging with extra-files disease.json\n",
    "            model_info = mlflow.pytorch.log_model(\n",
    "                pytorch_model=model,\n",
    "                #name=f\"{MODEL_NAME}_{DATASET_NAME}_ep{epochs}\",\n",
    "                registered_model_name=f\"{MODEL_NAME}_{DATASET_NAME}_ep{epochs}\",\n",
    "                extra_files=[ str(path) ]\n",
    "            )\n",
    "            print(model_info)\n",
    "\n",
    "        print(\"\\n--- Metrics and model logged into MLflow ---\\n\")\n",
    "\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bdd528d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"colomerus_vitis\": \"Erinose\",\n",
      "    \"elsinoe_ampelina\": \"Anthracnose\",\n",
      "    \"erysiphe_necator\": \"Oïdium\",\n",
      "    \"guignardia_bidwellii\": \"Pourriture_noire\",\n",
      "    \"phaeomoniella_chlamydospora\": \"Esca\",\n",
      "    \"plasmopara_viticola\": \"Mildiou\",\n",
      "    \"sain\": \"Pas de maladie\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DISEASES = {\n",
    "    \"colomerus_vitis\" : \"erinose\",\n",
    "    \"elsinoe_ampelina\" : \"anthracnose\",\n",
    "    \"erysiphe_necator\":\"oidium\",\n",
    "    \"guignardia_bidwellii\" : \"pourriture_noire\",\n",
    "    \"phaeomoniella_chlamydospora\" : \"esca\",\n",
    "    \"plasmopara_viticola\":\"mildiou\",\n",
    "    \"sain\" : \"sain\"\n",
    "    }\n",
    "'''\n",
    "print(json.dumps(DISEASES, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfe273d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.8607, Acc: 0.6912, Val Loss: 0.4437, Val Acc: 0.8389\n",
      "Epoch [2/30], Loss: 0.3851, Acc: 0.8781, Val Loss: 0.2918, Val Acc: 0.8886\n",
      "Epoch [3/30], Loss: 0.2707, Acc: 0.9199, Val Loss: 0.1677, Val Acc: 0.9360\n",
      "Epoch [4/30], Loss: 0.1900, Acc: 0.9358, Val Loss: 0.1528, Val Acc: 0.9384\n",
      "Epoch [5/30], Loss: 0.1431, Acc: 0.9567, Val Loss: 0.1424, Val Acc: 0.9502\n",
      "Epoch [6/30], Loss: 0.1440, Acc: 0.9574, Val Loss: 0.1757, Val Acc: 0.9408\n",
      "Epoch [7/30], Loss: 0.1252, Acc: 0.9567, Val Loss: 0.1631, Val Acc: 0.9455\n",
      "Epoch [8/30], Loss: 0.0941, Acc: 0.9726, Val Loss: 0.1220, Val Acc: 0.9573\n",
      "Epoch [9/30], Loss: 0.0995, Acc: 0.9711, Val Loss: 0.1332, Val Acc: 0.9479\n",
      "Epoch [10/30], Loss: 0.1012, Acc: 0.9668, Val Loss: 0.0781, Val Acc: 0.9692\n",
      "Epoch [11/30], Loss: 0.0565, Acc: 0.9885, Val Loss: 0.0720, Val Acc: 0.9810\n",
      "Epoch [12/30], Loss: 0.0663, Acc: 0.9791, Val Loss: 0.0834, Val Acc: 0.9858\n",
      "Epoch [13/30], Loss: 0.0421, Acc: 0.9877, Val Loss: 0.1082, Val Acc: 0.9621\n",
      "Epoch [14/30], Loss: 0.0484, Acc: 0.9856, Val Loss: 0.0596, Val Acc: 0.9787\n",
      "Epoch [15/30], Loss: 0.0632, Acc: 0.9776, Val Loss: 0.0777, Val Acc: 0.9692\n",
      "Epoch [16/30], Loss: 0.0466, Acc: 0.9834, Val Loss: 0.0765, Val Acc: 0.9787\n",
      "Epoch [17/30], Loss: 0.0554, Acc: 0.9848, Val Loss: 0.0856, Val Acc: 0.9716\n",
      "Epoch [18/30], Loss: 0.0344, Acc: 0.9892, Val Loss: 0.0691, Val Acc: 0.9763\n",
      "Epoch [19/30], Loss: 0.0462, Acc: 0.9906, Val Loss: 0.0575, Val Acc: 0.9858\n",
      "Epoch [20/30], Loss: 0.0534, Acc: 0.9913, Val Loss: 0.0777, Val Acc: 0.9787\n",
      "Epoch [21/30], Loss: 0.0636, Acc: 0.9805, Val Loss: 0.0825, Val Acc: 0.9763\n",
      "Epoch [22/30], Loss: 0.0683, Acc: 0.9769, Val Loss: 0.0827, Val Acc: 0.9668\n",
      "Epoch [23/30], Loss: 0.0533, Acc: 0.9841, Val Loss: 0.0449, Val Acc: 0.9882\n"
     ]
    }
   ],
   "source": [
    "# Train the model and store the training history\n",
    "history = train(model, train_loader, val_loader, test_loader, criterion, optimizer, experiment, epochs=30, patience=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe11d29",
   "metadata": {},
   "source": [
    "### Visualization of the learning process ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49551d62",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from plotly import graph_objects as go\n",
    "color_chart = [\"#4B9AC7\", \"#4BE8E0\", \"#9DD4F3\", \"#97FBF6\", \"#2A7FAF\", \"#23B1AB\", \"#0E3449\", \"#015955\"]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "                      go.Scatter(\n",
    "                          y=history[\"loss\"],\n",
    "                          name=\"Training loss\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[0]\n",
    "                          )),\n",
    "                      go.Scatter(\n",
    "                          y=history[\"val_loss\"],\n",
    "                          name=\"Validation loss\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[1]\n",
    "                          ))\n",
    "])\n",
    "fig.update_layout(\n",
    "    title='Training and val loss across epochs',\n",
    "    xaxis_title='epochs',\n",
    "    yaxis_title='Cross Entropy'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bee8a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "color_chart = [\"#4B9AC7\", \"#4BE8E0\", \"#9DD4F3\", \"#97FBF6\", \"#2A7FAF\", \"#23B1AB\", \"#0E3449\", \"#015955\"]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "                      go.Scatter(\n",
    "                          y=history[\"accuracy\"],\n",
    "                          name=\"Training Accuracy\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[0]\n",
    "                          )),\n",
    "                      go.Scatter(\n",
    "                          y=history[\"val_accuracy\"],\n",
    "                          name=\"Validation Accuracy\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[1]\n",
    "                          ))\n",
    "])\n",
    "fig.update_layout(\n",
    "    title='Training and val Accuracy across epochs',\n",
    "    xaxis_title='epochs',\n",
    "    yaxis_title='Cross Entropy'\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vitiscan_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
