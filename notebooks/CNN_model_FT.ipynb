{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec7fb97b",
   "metadata": {},
   "source": [
    "# **--- CNN MODEL ---** #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d3b0f5",
   "metadata": {},
   "source": [
    "## **I. Libraries import** ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a101012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import mlflow\n",
    "import tempfile\n",
    "import boto3\n",
    "\n",
    "# Torch ------------------\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models,datasets\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Metrics ------------------\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Visualization ---------\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1abf04",
   "metadata": {},
   "source": [
    "We select the appropriate torch device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3b28c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "#device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():  # Apple M1/M2/M3\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a30955",
   "metadata": {},
   "source": [
    "## **II. Images import and processing** ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ffdbe",
   "metadata": {},
   "source": [
    "### Load dataset ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9148d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inrae\n",
      "../data-inrae\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME=os.getenv(\"DATASET_NAME\",\"inrae\")\n",
    "DATASET_DIR=Path(f\"../data-{DATASET_NAME}\")\n",
    "print(DATASET_NAME)\n",
    "print(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "694588e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sain: 350 images\n",
      "Nothing to delete\n"
     ]
    }
   ],
   "source": [
    "#  We randomly reduce the amount of images in the \"healthy\" class that is too represented : \n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "TARGET_NB = 350\n",
    "sain = Path(f\"{DATASET_DIR}/raw_data_inrae/sain\")\n",
    "\n",
    "images = list(sain.glob(\"*\"))\n",
    "n_images = len(images)\n",
    "\n",
    "print(f\"{sain.name}: {n_images} images\")\n",
    "\n",
    "if n_images <= TARGET_NB:\n",
    "    print(\"Nothing to delete\")\n",
    "else:\n",
    "    images_to_delete = random.sample(\n",
    "        images, n_images - TARGET_NB\n",
    "    )\n",
    "\n",
    "    for img in images_to_delete:\n",
    "        img.unlink()\n",
    "\n",
    "    print(f\"{len(images_to_delete)} images deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3ca296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Clean the target folder\n",
    "organized_data = Path(f\"../{DATASET_DIR}/organized_data_inrae\")\n",
    "\n",
    "if organized_data.exists():\n",
    "    shutil.rmtree(organized_data)\n",
    "\n",
    "\n",
    "organized_data.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --------------   Divide the dataset into Train, Val and Test -------------------\n",
    "random.seed(42)\n",
    "\n",
    "raw_data_path = Path(f\"{DATASET_DIR}/raw_data_inrae\")\n",
    "organized_data = Path(f\"{DATASET_DIR}/organized_data_inrae\")\n",
    "\n",
    "SPLITS = {\n",
    "    \"train\": 0.7,\n",
    "    \"val\": 0.15,\n",
    "    \"test\": 0.15\n",
    "}\n",
    "\n",
    "# Create folder structure\n",
    "for split in SPLITS:\n",
    "    for class_dir in raw_data_path.iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            (organized_data / split / class_dir.name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Split images\n",
    "for class_dir in raw_data_path.iterdir():\n",
    "    if not class_dir.is_dir():\n",
    "        continue\n",
    "\n",
    "    images = [\n",
    "        img for img in class_dir.iterdir()\n",
    "        if img.suffix.lower() in {\".jpg\", \".jpeg\", \".png\"}\n",
    "    ]\n",
    "\n",
    "    random.shuffle(images)\n",
    "\n",
    "    n_total = len(images)\n",
    "    n_train = int(n_total * SPLITS[\"train\"])\n",
    "    n_val = int(n_total * SPLITS[\"val\"])\n",
    "\n",
    "    train_imgs = images[:n_train]\n",
    "    val_imgs = images[n_train:n_train + n_val]\n",
    "    test_imgs = images[n_train + n_val:]\n",
    "\n",
    "    for img in train_imgs:\n",
    "        shutil.copy(img, organized_data / \"train\" / class_dir.name / img.name)\n",
    "\n",
    "    for img in val_imgs:\n",
    "        shutil.copy(img, organized_data / \"val\" / class_dir.name / img.name)\n",
    "\n",
    "    for img in test_imgs:\n",
    "        shutil.copy(img, organized_data / \"test\" / class_dir.name / img.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a13b510f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN\n",
      "  colomerus_vitis      : 119\n",
      "  erysiphe_necator     : 91\n",
      "  guignardia_bidwellii : 169\n",
      "  phaeomoniella_chlamydospora : 82\n",
      "  plasmopara_viticola  : 345\n",
      "  sain                 : 318\n",
      "  elsinoe_ampelina     : 262\n",
      "  TOTAL train          : 1386\n",
      "\n",
      "VAL\n",
      "  colomerus_vitis      : 34\n",
      "  erysiphe_necator     : 27\n",
      "  guignardia_bidwellii : 51\n",
      "  phaeomoniella_chlamydospora : 25\n",
      "  plasmopara_viticola  : 106\n",
      "  sain                 : 98\n",
      "  elsinoe_ampelina     : 81\n",
      "  TOTAL val            : 422\n",
      "\n",
      "TEST\n",
      "  colomerus_vitis      : 38\n",
      "  erysiphe_necator     : 31\n",
      "  guignardia_bidwellii : 54\n",
      "  phaeomoniella_chlamydospora : 25\n",
      "  plasmopara_viticola  : 105\n",
      "  sain                 : 98\n",
      "  elsinoe_ampelina     : 83\n",
      "  TOTAL test           : 434\n"
     ]
    }
   ],
   "source": [
    "# We check the size of the new folders\n",
    "\n",
    "ROOT_DIR = Path(f\"{DATASET_DIR}/organized_data_inrae\")\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    print(f\"\\n{split.upper()}\")\n",
    "    total = 0\n",
    "\n",
    "    for class_dir in (ROOT_DIR / split).iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            n_files = len(list(class_dir.glob(\"*\")))\n",
    "            total += n_files\n",
    "            print(f\"  {class_dir.name:<20} : {n_files}\")\n",
    "\n",
    "    print(f\"  TOTAL {split:<14} : {total}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e4a4c",
   "metadata": {},
   "source": [
    "### Pipeline for data transformation ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5c1385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations of the train set with data augmentation\n",
    "\n",
    "transform_train= transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=(-45,+45)),\n",
    "    transforms.ToTensor(),        \n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225])  # Normalization with the values for the pre-trained Resnet model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29580af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the path to the training dataset\n",
    "data_train = Path(f\"{DATASET_DIR}/organized_data_inrae/train\")\n",
    "\n",
    "# Load dataset with ImageFolder\n",
    "train_dataset = ImageFolder(root=data_train, transform=transform_train)\n",
    "\n",
    "# Get class names\n",
    "class_names = train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006dc0c",
   "metadata": {},
   "source": [
    "We build a dictionnary with translations and we send a JSON file to our S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ba2c9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"colomerus_vitis\": \"Erinose\",\n",
      "    \"elsinoe_ampelina\": \"Anthracnose\",\n",
      "    \"erysiphe_necator\": \"O√Ødium\",\n",
      "    \"guignardia_bidwellii\": \"Pourriture_noire\",\n",
      "    \"phaeomoniella_chlamydospora\": \"Esca\",\n",
      "    \"plasmopara_viticola\": \"Mildiou\",\n",
      "    \"sain\": \"Pas de maladie\"\n",
      "}\n",
      "/tmp/tmpb4f_pase/disease-inrae.json\n",
      "Disease dictionnary uploaded to : s3://aws-s3-mlflow/vitiscan-data/disease-inrae.json\n"
     ]
    }
   ],
   "source": [
    "translations = [\n",
    "    \"Erinose\",\n",
    "    \"Anthracnose\",\n",
    "    \"O√Ødium\",\n",
    "    \"Pourriture_noire\",\n",
    "    \"Esca\",\n",
    "    \"Mildiou\",\n",
    "    \"Pas de maladie\"\n",
    "]\n",
    "\n",
    "DISEASES = {}\n",
    "for c,t in zip(class_names, translations):\n",
    "    DISEASES[c] = t\n",
    "\n",
    "print(json.dumps(DISEASES, indent=4, ensure_ascii=False))\n",
    "\n",
    "S3_BUCKET_NAME= os.getenv('S3_BUCKET_NAME', \"aws-s3-mlflow\")\n",
    "\n",
    "# Temporary file for saving the confusion matrices\n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        s3 = boto3.client('s3')\n",
    "        path = Path(tmp_dir, f\"disease-{DATASET_NAME}.json\")\n",
    "        print(str(path))\n",
    "        with path.open('w') as f:\n",
    "            json.dump(DISEASES, f)\n",
    "        dest_file_name = f'vitiscan-data/disease-{DATASET_NAME}.json'\n",
    "        s3.upload_file(Bucket=S3_BUCKET_NAME, Filename=str(path), Key=dest_file_name)\n",
    "        s3.close()\n",
    "        print(f\"Disease dictionnary uploaded to : s3://{S3_BUCKET_NAME}/{dest_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8841b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Transformation pipeline without data augmentation for the validation and the test set\n",
    "\n",
    "transform= transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Redimensionnement √† 224x224\n",
    "    transforms.ToTensor(),          # Conversion en tenseur\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225])  # Normalisation avec les valeurs du mod√®le pr√©-entra√Æn√© Resnet\n",
    "])\n",
    "\n",
    "# Create the path to the training dataset\n",
    "data_val = Path(f\"{DATASET_DIR}/organized_data_inrae/val\")\n",
    "data_test = Path(f\"{DATASET_DIR}/organized_data_inrae/test\")\n",
    "\n",
    "# Load dataset with ImageFolder\n",
    "val_dataset = ImageFolder(root=data_val,transform=transform)\n",
    "test_dataset = ImageFolder(root=data_test, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "174b6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b7e983",
   "metadata": {},
   "source": [
    "## **Fine tuning (Resnet18)** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a677dfea",
   "metadata": {},
   "source": [
    "### Preparing the MLFlow tracking ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9da841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tracking URI to your Hugging Face application\n",
    "MLFLOW_URI=os.getenv('MLFLOW_URI',\"https://gviel-mlflow37.hf.space/\")\n",
    "mlflow.set_tracking_uri(os.environ[\"MLFLOW_URI\"])\n",
    "\n",
    "# Set experiment's info\n",
    "EXPERIMENT_NAME= os.getenv('EXPERIMENT_NAME',\"Vitiscan_CNN_MLFlow_FineTuning\")+\"FINE_TUNING\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# Get our experiment info\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d57000",
   "metadata": {},
   "source": [
    "### Importing a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb1666c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#model = models.resnet18(pretrained=True) # pour version <0.13.0\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT) # pour version >=0.13.0\n",
    "model = model.to(device)\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c762ac3b",
   "metadata": {},
   "source": [
    "### Replace classification layer to adapt the model to our features ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ff3244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes=len(train_dataset.classes)\n",
    "model.fc = nn.Linear(model.fc.in_features, nb_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb65f752",
   "metadata": {},
   "source": [
    "### Freeze the feature extraction layers ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c5d072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We freeze the entire network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# We unfreeze only the last Resnet18 blocks of layers (called \"Layer4 for resnet18\")\n",
    "# AND we unfreeze the classifier fc\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer4\" in name or \"fc\" in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dec27343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 7]                    --\n",
       "‚îú‚îÄConv2d: 1-1                            [1, 64, 112, 112]         (9,408)\n",
       "‚îú‚îÄBatchNorm2d: 1-2                       [1, 64, 112, 112]         (128)\n",
       "‚îú‚îÄReLU: 1-3                              [1, 64, 112, 112]         --\n",
       "‚îú‚îÄMaxPool2d: 1-4                         [1, 64, 56, 56]           --\n",
       "‚îú‚îÄSequential: 1-5                        [1, 64, 56, 56]           --\n",
       "‚îÇ    ‚îî‚îÄBasicBlock: 2-1                   [1, 64, 56, 56]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-1                  [1, 64, 56, 56]           (36,864)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-2             [1, 64, 56, 56]           (128)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-3                    [1, 64, 56, 56]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-4                  [1, 64, 56, 56]           (36,864)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-5             [1, 64, 56, 56]           (128)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-6                    [1, 64, 56, 56]           --\n",
       "‚îÇ    ‚îî‚îÄBasicBlock: 2-2                   [1, 64, 56, 56]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-7                  [1, 64, 56, 56]           (36,864)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-8             [1, 64, 56, 56]           (128)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-9                    [1, 64, 56, 56]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-10                 [1, 64, 56, 56]           (36,864)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-11            [1, 64, 56, 56]           (128)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-12                   [1, 64, 56, 56]           --\n",
       "‚îú‚îÄSequential: 1-6                        [1, 128, 28, 28]          --\n",
       "‚îÇ    ‚îî‚îÄBasicBlock: 2-3                   [1, 128, 28, 28]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-13                 [1, 128, 28, 28]          (73,728)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-14            [1, 128, 28, 28]          (256)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-15                   [1, 128, 28, 28]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-16                 [1, 128, 28, 28]          (147,456)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-17            [1, 128, 28, 28]          (256)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-18             [1, 128, 28, 28]          (8,448)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-19                   [1, 128, 28, 28]          --\n",
       "‚îÇ    ‚îî‚îÄBasicBlock: 2-4                   [1, 128, 28, 28]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-20                 [1, 128, 28, 28]          (147,456)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-21            [1, 128, 28, 28]          (256)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-22                   [1, 128, 28, 28]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-23                 [1, 128, 28, 28]          (147,456)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-24            [1, 128, 28, 28]          (256)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-25                   [1, 128, 28, 28]          --\n",
       "‚îú‚îÄSequential: 1-7                        [1, 256, 14, 14]          --\n",
       "‚îÇ    ‚îî‚îÄBasicBlock: 2-5                   [1, 256, 14, 14]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-26                 [1, 256, 14, 14]          (294,912)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-27            [1, 256, 14, 14]          (512)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-28                   [1, 256, 14, 14]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-29                 [1, 256, 14, 14]          (589,824)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-30            [1, 256, 14, 14]          (512)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-31             [1, 256, 14, 14]          (33,280)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-32                   [1, 256, 14, 14]          --\n",
       "‚îÇ    ‚îî‚îÄBasicBlock: 2-6                   [1, 256, 14, 14]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-33                 [1, 256, 14, 14]          (589,824)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-34            [1, 256, 14, 14]          (512)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-35                   [1, 256, 14, 14]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-36                 [1, 256, 14, 14]          (589,824)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-37            [1, 256, 14, 14]          (512)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-38                   [1, 256, 14, 14]          --\n",
       "‚îú‚îÄSequential: 1-8                        [1, 512, 7, 7]            --\n",
       "‚îÇ    ‚îî‚îÄBasicBlock: 2-7                   [1, 512, 7, 7]            --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-39                 [1, 512, 7, 7]            1,179,648\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-40            [1, 512, 7, 7]            1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-41                   [1, 512, 7, 7]            --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-42                 [1, 512, 7, 7]            2,359,296\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-43            [1, 512, 7, 7]            1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-44             [1, 512, 7, 7]            132,096\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-45                   [1, 512, 7, 7]            --\n",
       "‚îÇ    ‚îî‚îÄBasicBlock: 2-8                   [1, 512, 7, 7]            --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-46                 [1, 512, 7, 7]            2,359,296\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-47            [1, 512, 7, 7]            1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-48                   [1, 512, 7, 7]            --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-49                 [1, 512, 7, 7]            2,359,296\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-50            [1, 512, 7, 7]            1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-51                   [1, 512, 7, 7]            --\n",
       "‚îú‚îÄAdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --\n",
       "‚îú‚îÄLinear: 1-10                           [1, 7]                    3,591\n",
       "==========================================================================================\n",
       "Total params: 11,180,103\n",
       "Trainable params: 8,397,319\n",
       "Non-trainable params: 2,782,784\n",
       "Total mult-adds (Units.GIGABYTES): 1.81\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 39.74\n",
       "Params size (MB): 44.72\n",
       "Estimated Total Size (MB): 85.06\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print model summary\n",
    "summary(model, input_size=(1, 3, 224, 224))  # (batch_size, input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827dc7ed",
   "metadata": {},
   "source": [
    "### Defining the cost function and optimizer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c085ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device) \n",
    "\n",
    "learning_rate=0.0001\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=0.0001 # Ridge regulation to avoid overfitting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94632022",
   "metadata": {},
   "source": [
    "### Train the model ###\n",
    "\n",
    "We check an output on the fist batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "222aa307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2581,  0.0351, -0.1942, -0.3909, -0.1313, -0.6100, -0.8514],\n",
       "        [-0.0844,  0.7455, -0.1052, -0.3020,  0.0630, -0.4493, -0.6740],\n",
       "        [ 0.8736,  0.0063,  0.0579, -0.0556, -0.0840, -0.3114, -0.7350],\n",
       "        [ 0.5816,  0.9475, -0.3270,  0.7279,  0.5075, -0.1729, -1.0160],\n",
       "        [ 0.4482,  0.6876,  0.1040, -0.4828, -0.1668, -1.2552, -0.0777],\n",
       "        [ 0.9259,  0.8473,  0.2512, -0.5780, -0.6492, -0.4590, -0.4265],\n",
       "        [ 0.9657,  0.7889,  0.4453, -0.2684,  0.6325, -0.7888, -0.2608],\n",
       "        [ 0.6828,  0.5779, -0.1858, -0.1814,  0.3079, -0.4639, -0.8463],\n",
       "        [ 0.1773,  0.2449, -0.3121,  0.0227,  0.6181, -0.6690, -0.9461],\n",
       "        [ 0.4295,  0.7947, -0.3575, -0.2658, -0.0257, -0.4239, -0.1446],\n",
       "        [ 1.3525,  0.6106, -0.1213, -0.3091,  0.7801, -0.4456,  0.1556],\n",
       "        [ 1.0808,  1.1653, -0.0967,  0.2634,  0.3628, -0.9215, -1.5761],\n",
       "        [-0.0271,  0.1723, -0.0329, -0.8453,  0.5059, -0.6402, -0.5270],\n",
       "        [ 0.4036,  0.5703, -0.1502,  0.8694, -0.3445, -0.3796, -0.9066],\n",
       "        [ 0.7013,  0.7568,  0.0431, -0.1528, -0.8036, -0.5008, -1.2104],\n",
       "        [ 0.3759,  0.6929,  0.3198,  0.0310,  0.1720, -0.1204, -0.6676],\n",
       "        [ 0.8822,  0.2633,  0.1437,  0.3355,  0.2179, -0.3714, -0.2777],\n",
       "        [ 0.4189,  0.3700, -0.5375,  0.3656, -0.2175, -0.4884, -0.4487],\n",
       "        [-0.2998,  0.4958, -0.0321, -0.3150, -0.0562, -0.0265, -1.1053],\n",
       "        [ 1.3258,  0.4117,  0.2383,  0.1820,  0.9709, -0.7366, -0.3260],\n",
       "        [ 0.1526,  0.2635,  0.2817,  0.3306,  0.3457,  0.3299, -1.3618],\n",
       "        [ 0.9647,  0.7320, -0.4427,  0.2866,  0.0502,  0.0529, -0.9770],\n",
       "        [ 1.1168,  0.5235,  0.1070, -0.2367,  0.0497, -0.8742, -0.2417],\n",
       "        [ 0.9179,  0.5207,  0.4050,  0.0506, -0.0157, -2.1154, -0.5056],\n",
       "        [ 0.2361,  0.7649,  0.2428,  0.0182,  0.3113, -0.9821, -0.4890],\n",
       "        [ 0.4160,  0.9893,  1.2820, -0.0262,  0.2580, -0.5870, -0.6022],\n",
       "        [-0.2144, -0.2853, -0.2106, -0.2662, -0.0520, -0.8791, -0.3305],\n",
       "        [ 1.2952,  0.4740, -0.0966,  0.4376, -0.1069, -0.6801, -0.4317],\n",
       "        [ 0.4950,  1.0210,  0.0613,  0.1944,  0.0324, -0.2207, -1.1801],\n",
       "        [ 0.5443,  0.3980, -0.5047,  0.1748, -0.1368, -0.3348, -0.7882],\n",
       "        [ 0.0382,  0.6526,  0.1278,  0.6246,  0.1210, -0.6362, -1.3269],\n",
       "        [ 1.0565,  0.1549, -0.4850,  0.1655,  0.2001, -0.7208, -0.5881]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model outputs\n",
    "image, label = next(iter(train_loader))\n",
    "image=image.to(device)\n",
    "model=model.to(device)\n",
    "logit= model(image) # Resnet18 output\n",
    "logit\n",
    "# Attention il faut que l'image et le modele aient le m√™me device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6900526c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['colomerus_vitis',\n",
       " 'elsinoe_ampelina',\n",
       " 'erysiphe_necator',\n",
       " 'guignardia_bidwellii',\n",
       " 'phaeomoniella_chlamydospora',\n",
       " 'plasmopara_viticola',\n",
       " 'sain']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0be446dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_test(model, test_loader, device):\n",
    "    \n",
    "    model.eval() # Mode √©valuation\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logit = model(images)\n",
    "            preds = logit.argmax(dim=1)\n",
    "            \n",
    "            # Compter l'exactitude\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            \n",
    "            # Collecter pour les m√©triques de Scikit-learn\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    \n",
    "    test_accuracy = correct / total\n",
    "    return test_accuracy, y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb409d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_confusion_matrix(dataset_type:str, y_true:list, y_pred:list):\n",
    "    ''' Generate a Confusion Matrix and log it into MLFlow'''\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "            cm_test = confusion_matrix(y_true, y_pred)\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.xticks(ticks= range(nb_classes),labels=class_names, rotation=45, ha=\"right\")\n",
    "            plt.yticks(ticks= range(nb_classes),labels=class_names,rotation=0)\n",
    "            plt.ylabel('True class')\n",
    "            plt.xlabel('Predicted class')\n",
    "            plt.title(f'Confusion matrix - {dataset_type}')\n",
    "            plt.tight_layout()\n",
    "            path = str(Path(tmp_dir, f\"confusion_matrix_{dataset_type}.png\"))\n",
    "            plt.savefig(path, dpi=150)\n",
    "            mlflow.log_artifact(path)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b130c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_precision_recall_f1_score(dataset_type:str, y_true:list, y_pred:list):\n",
    "    '''\n",
    "        Compute various scores and log them into MLFlow\n",
    "        Return the scores in a dict\n",
    "    '''\n",
    "    results = {}\n",
    "    average_modes=['weighted', 'macro']\n",
    "    for avg_mode in average_modes:\n",
    "        results[avg_mode] = {}\n",
    "        for score in ['precision', 'recall', 'f1']:\n",
    "            # --- Precision ---\n",
    "            if score == 'precision':\n",
    "                metric_value = precision_score(y_true, y_pred, average=avg_mode, zero_division=0)\n",
    "            elif score == 'recall':\n",
    "                metric_value = recall_score(y_true, y_pred, average=avg_mode, zero_division=0)\n",
    "            elif score == 'f1':\n",
    "                metric_value = f1_score(y_true, y_pred, average=avg_mode, zero_division=0)\n",
    "            \n",
    "            metric_name = f\"{dataset_type.capitalize()}_{score}_{avg_mode}\"\n",
    "            results[avg_mode][score] = metric_value\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c354fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for a PyTorch model\n",
    "def train(model, train_loader, val_loader, test_loader, criterion, optimizer, experiment, epochs=20, patience=5):\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # We start a MLflow run\n",
    "    with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "\n",
    "        # Logging Pytorch parameters into MLflow \n",
    "        params = {\n",
    "            \"optimizer\": type(optimizer).__name__,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"epochs\": epochs,\n",
    "            \"criterion\": type(criterion).__name__,\n",
    "            \"model_architecture\": type(model).__name__,\n",
    "            \"training_device\": str(device),\n",
    "            \"weight_decay\": optimizer.param_groups[0][\"weight_decay\"]\n",
    "        }\n",
    "        \n",
    "        mlflow.log_params(params=params)\n",
    "        mlflow.pytorch.autolog()\n",
    "\n",
    "        # Dictionary to store loss and accuracy values for each epoch\n",
    "        history = {\n",
    "            'loss': [],\n",
    "            'val_loss': [],\n",
    "            'accuracy': [],\n",
    "            'val_accuracy': []\n",
    "        }\n",
    "\n",
    "        # ------------------- TRAINING LOOP -------------------\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_loss, correct = 0, 0  \n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logit = model(images)\n",
    "                loss = criterion(logit, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                correct += (logit.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "            train_loss = total_loss / len(train_loader)\n",
    "            train_acc = correct / len(train_loader.dataset)\n",
    "\n",
    "            # ------------------- VALIDATION LOOP -------------------\n",
    "            model.eval()\n",
    "            val_loss, val_correct = 0, 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    logit = model(images)\n",
    "                    loss = criterion(logit, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    val_correct += (logit.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "            # --- Save metrics ---\n",
    "            history['loss'].append(train_loss)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['accuracy'].append(train_acc)\n",
    "            history['val_accuracy'].append(val_acc)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{epochs}], \"\n",
    "                f\"Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, \"\n",
    "                f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "            #  Logging metrics\n",
    "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "            mlflow.log_metric(\"train_accuracy\", train_acc, step=epoch)\n",
    "            mlflow.log_metric(\"validation_loss\", val_loss, step=epoch)\n",
    "            mlflow.log_metric(\"validation_accuracy\", val_acc, step=epoch)\n",
    "\n",
    "            # --- Early stopping check ---\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = model.state_dict()  # save the best model\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                    model.load_state_dict(best_model_state)  # restore best model\n",
    "                    break\n",
    "                \n",
    "\n",
    "        # ================= FINAL VALIDATION EVALUATION =================\n",
    "        print(\"\\n--- Final evaluation on the VALIDATION set ---\\n\")\n",
    "\n",
    "        y_true_val, y_pred_val = [], []\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                logit = model(images)\n",
    "                preds = logit.argmax(dim=1)\n",
    "\n",
    "                y_true_val.extend(labels.cpu().numpy())\n",
    "                y_pred_val.extend(preds.cpu().numpy())\n",
    "\n",
    "        # --- Compute scores (Validation) ---\n",
    "        val_scores = log_precision_recall_f1_score('validation', y_true_val, y_pred_val)\n",
    "        for mode in val_scores.keys():\n",
    "            for score in val_scores[mode].keys():\n",
    "                print(f\"{score.capitalize()} Validation ({mode}): {val_scores[mode][score]:.4f}\")\n",
    "                \n",
    "        # Final metrics logging\n",
    "        mlflow.log_metric(\"final_validation_accuracy\", val_acc)\n",
    "        print(f\"Final Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        # --- Confusion Matrix (Validation) ---\n",
    "        log_confusion_matrix('VALIDATION', y_true_val, y_pred_val)\n",
    "\n",
    "        # ================= FINAL TEST EVALUATION =================\n",
    "        print(\"\\n--- Final evaluation on the TEST set ---\\n\")\n",
    "        test_acc, y_true_test, y_pred_test = evaluate_model_on_test(model, test_loader, device)\n",
    "\n",
    "        # --- Compute scores (test) ---\n",
    "        test_scores = log_precision_recall_f1_score('test', y_true_test, y_pred_test)\n",
    "        for mode in test_scores.keys():\n",
    "            for score in test_scores[mode].keys():\n",
    "                print(f\"{score.capitalize()} Validation ({mode}): {test_scores[mode][score]:.4f}\")\n",
    "\n",
    "        # --- Accuracy (test) ---\n",
    "        mlflow.log_metric(\"Test_accuracy\", test_acc)\n",
    "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "        # --- Confusion Matrix (Test) ---\n",
    "        log_confusion_matrix('TEST', y_true_test, y_pred_test)\n",
    "\n",
    "        # --- Model logging ---\n",
    "        mlflow.pytorch.log_model(\n",
    "            pytorch_model=model,\n",
    "            artifact_path=\"Resnet18\",\n",
    "            registered_model_name=f\"{params['model_architecture']}\"\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- Metrics and model logged into MLflow ---\\n\")\n",
    "\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6bdd528d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"colomerus_vitis\": \"Erinose\",\n",
      "    \"elsinoe_ampelina\": \"Anthracnose\",\n",
      "    \"erysiphe_necator\": \"O√Ødium\",\n",
      "    \"guignardia_bidwellii\": \"Pourriture_noire\",\n",
      "    \"phaeomoniella_chlamydospora\": \"Esca\",\n",
      "    \"plasmopara_viticola\": \"Mildiou\",\n",
      "    \"sain\": \"Pas de maladie\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DISEASES = {\n",
    "    \"colomerus_vitis\" : \"erinose\",\n",
    "    \"elsinoe_ampelina\" : \"anthracnose\",\n",
    "    \"erysiphe_necator\":\"oidium\",\n",
    "    \"guignardia_bidwellii\" : \"pourriture_noire\",\n",
    "    \"phaeomoniella_chlamydospora\" : \"esca\",\n",
    "    \"plasmopara_viticola\":\"mildiou\",\n",
    "    \"sain\" : \"sain\"\n",
    "    }\n",
    "'''\n",
    "print(json.dumps(DISEASES, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "acfe273d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 0.2445, Acc: 0.9192, Val Loss: 0.1728, Val Acc: 0.9455\n",
      "Epoch [2/3], Loss: 0.1852, Acc: 0.9488, Val Loss: 0.1803, Val Acc: 0.9384\n",
      "Epoch [3/3], Loss: 0.1654, Acc: 0.9466, Val Loss: 0.1388, Val Acc: 0.9526\n",
      "\n",
      "--- Final evaluation on the VALIDATION set ---\n",
      "\n",
      "Precision Validation (weighted): 0.9571\n",
      "Recall Validation (weighted): 0.9526\n",
      "F1 Validation (weighted): 0.9511\n",
      "Precision Validation (macro): 0.9630\n",
      "Recall Validation (macro): 0.9365\n",
      "F1 Validation (macro): 0.9454\n",
      "Final Validation Accuracy: 0.9526\n",
      "\n",
      "--- Final evaluation on the TEST set ---\n",
      "\n",
      "Precision Validation (weighted): 0.9542\n",
      "Recall Validation (weighted): 0.9493\n",
      "F1 Validation (weighted): 0.9476\n",
      "Precision Validation (macro): 0.9634\n",
      "Recall Validation (macro): 0.9406\n",
      "F1 Validation (macro): 0.9476\n",
      "Test Accuracy: 0.9493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/19 19:48:30 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "Registered model 'ResNet' already exists. Creating a new version of this model...\n",
      "2025/12/19 19:49:01 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ResNet, version 24\n",
      "Created version '24' of model 'ResNet'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Metrics and model logged into MLflow ---\n",
      "\n",
      "üèÉ View run debonair-crab-123 at: https://gviel-mlflow37.hf.space/#/experiments/5/runs/2cdb6b01fb8b4355870cfe2d05697eaf\n",
      "üß™ View experiment at: https://gviel-mlflow37.hf.space/#/experiments/5\n"
     ]
    }
   ],
   "source": [
    "# Train the model and store the training history\n",
    "history = train(model, train_loader, val_loader, test_loader, criterion, optimizer, experiment, epochs=3, patience=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe11d29",
   "metadata": {},
   "source": [
    "### Visualization of the learning process ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49551d62",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from plotly import graph_objects as go\n",
    "color_chart = [\"#4B9AC7\", \"#4BE8E0\", \"#9DD4F3\", \"#97FBF6\", \"#2A7FAF\", \"#23B1AB\", \"#0E3449\", \"#015955\"]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "                      go.Scatter(\n",
    "                          y=history[\"loss\"],\n",
    "                          name=\"Training loss\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[0]\n",
    "                          )),\n",
    "                      go.Scatter(\n",
    "                          y=history[\"val_loss\"],\n",
    "                          name=\"Validation loss\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[1]\n",
    "                          ))\n",
    "])\n",
    "fig.update_layout(\n",
    "    title='Training and val loss across epochs',\n",
    "    xaxis_title='epochs',\n",
    "    yaxis_title='Cross Entropy'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bee8a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "color_chart = [\"#4B9AC7\", \"#4BE8E0\", \"#9DD4F3\", \"#97FBF6\", \"#2A7FAF\", \"#23B1AB\", \"#0E3449\", \"#015955\"]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "                      go.Scatter(\n",
    "                          y=history[\"accuracy\"],\n",
    "                          name=\"Training Accuracy\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[0]\n",
    "                          )),\n",
    "                      go.Scatter(\n",
    "                          y=history[\"val_accuracy\"],\n",
    "                          name=\"Validation Accuracy\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[1]\n",
    "                          ))\n",
    "])\n",
    "fig.update_layout(\n",
    "    title='Training and val Accuracy across epochs',\n",
    "    xaxis_title='epochs',\n",
    "    yaxis_title='Cross Entropy'\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vitiscan_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
