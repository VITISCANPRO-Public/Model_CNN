{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec7fb97b",
   "metadata": {},
   "source": [
    "# **--- CNN MODEL ---** #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d3b0f5",
   "metadata": {},
   "source": [
    "## **I. Libraries import** ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6a101012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import mlflow\n",
    "\n",
    "# Torch ------------------\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models,datasets\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Metrics ------------------\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Visualization ---------\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1abf04",
   "metadata": {},
   "source": [
    "We select the appropriate torch device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a3b28c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "#device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():  # Apple M1/M2/M3\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a30955",
   "metadata": {},
   "source": [
    "## **II. Images import and processing** ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ffdbe",
   "metadata": {},
   "source": [
    "### Load dataset ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c9148d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=os.getenv(\"DATASET_NAME\",\"inrae\")\n",
    "DATASET_DIR=Path(f\"../data-{DATASET_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "694588e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sain: 350 images\n",
      "Nothing to delete\n"
     ]
    }
   ],
   "source": [
    "#  We randomly reduce the amount of images in the \"healthy\" class that is too represented : \n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "TARGET_NB = 350\n",
    "sain = Path(f\"{DATASET_DIR}/raw_data_inrae/sain\")\n",
    "\n",
    "images = list(sain.glob(\"*\"))\n",
    "n_images = len(images)\n",
    "\n",
    "print(f\"{sain.name}: {n_images} images\")\n",
    "\n",
    "if n_images <= TARGET_NB:\n",
    "    print(\"Nothing to delete\")\n",
    "else:\n",
    "    images_to_delete = random.sample(\n",
    "        images, n_images - TARGET_NB\n",
    "    )\n",
    "\n",
    "    for img in images_to_delete:\n",
    "        img.unlink()\n",
    "\n",
    "    print(f\"{len(images_to_delete)} images deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e3ca296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Clean the target folder\n",
    "organized_data = Path(f\"{DATASET_DIR}/organized_data_inrae\")\n",
    "\n",
    "if organized_data.exists():\n",
    "    shutil.rmtree(organized_data)\n",
    "\n",
    "\n",
    "organized_data.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --------------   Divide the dataset into Train, Val and Test -------------------\n",
    "random.seed(42)\n",
    "\n",
    "raw_data_path = Path(f\"{DATASET_DIR}/raw_data_inrae\")\n",
    "organized_data = Path(f\"{DATASET_DIR}/organized_data_inrae\")\n",
    "\n",
    "SPLITS = {\n",
    "    \"train\": 0.7,\n",
    "    \"val\": 0.15,\n",
    "    \"test\": 0.15\n",
    "}\n",
    "\n",
    "# Create folder structure\n",
    "for split in SPLITS:\n",
    "    for class_dir in raw_data_path.iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            (organized_data / split / class_dir.name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Split images\n",
    "for class_dir in raw_data_path.iterdir():\n",
    "    if not class_dir.is_dir():\n",
    "        continue\n",
    "\n",
    "    images = [\n",
    "        img for img in class_dir.iterdir()\n",
    "        if img.suffix.lower() in {\".jpg\", \".jpeg\", \".png\"}\n",
    "    ]\n",
    "\n",
    "    random.shuffle(images)\n",
    "\n",
    "    n_total = len(images)\n",
    "    n_train = int(n_total * SPLITS[\"train\"])\n",
    "    n_val = int(n_total * SPLITS[\"val\"])\n",
    "\n",
    "    train_imgs = images[:n_train]\n",
    "    val_imgs = images[n_train:n_train + n_val]\n",
    "    test_imgs = images[n_train + n_val:]\n",
    "\n",
    "    for img in train_imgs:\n",
    "        shutil.copy(img, organized_data / \"train\" / class_dir.name / img.name)\n",
    "\n",
    "    for img in val_imgs:\n",
    "        shutil.copy(img, organized_data / \"val\" / class_dir.name / img.name)\n",
    "\n",
    "    for img in test_imgs:\n",
    "        shutil.copy(img, organized_data / \"test\" / class_dir.name / img.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a13b510f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN\n",
      "  guignardia_bidwellii : 129\n",
      "  elsinoe_ampelina     : 202\n",
      "  erysiphe_necator     : 71\n",
      "  sain                 : 244\n",
      "  plasmopara_viticola  : 260\n",
      "  phaeomoniella_chlamydospora : 65\n",
      "  colomerus_vitis      : 90\n",
      "  TOTAL train          : 1061\n",
      "\n",
      "VAL\n",
      "  guignardia_bidwellii : 27\n",
      "  elsinoe_ampelina     : 43\n",
      "  erysiphe_necator     : 15\n",
      "  sain                 : 52\n",
      "  plasmopara_viticola  : 55\n",
      "  phaeomoniella_chlamydospora : 14\n",
      "  colomerus_vitis      : 19\n",
      "  TOTAL val            : 225\n",
      "\n",
      "TEST\n",
      "  guignardia_bidwellii : 29\n",
      "  elsinoe_ampelina     : 44\n",
      "  erysiphe_necator     : 16\n",
      "  sain                 : 54\n",
      "  plasmopara_viticola  : 57\n",
      "  phaeomoniella_chlamydospora : 15\n",
      "  colomerus_vitis      : 20\n",
      "  TOTAL test           : 235\n"
     ]
    }
   ],
   "source": [
    "# We check the size of the new folders\n",
    "\n",
    "ROOT_DIR = Path(f\"{DATASET_DIR}/organized_data_inrae\")\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    print(f\"\\n{split.upper()}\")\n",
    "    total = 0\n",
    "\n",
    "    for class_dir in (ROOT_DIR / split).iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            n_files = len(list(class_dir.glob(\"*\")))\n",
    "            total += n_files\n",
    "            print(f\"  {class_dir.name:<20} : {n_files}\")\n",
    "\n",
    "    print(f\"  TOTAL {split:<14} : {total}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e4a4c",
   "metadata": {},
   "source": [
    "### Pipeline for data transformation ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a5c1385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations of the train set with data augmentation\n",
    "\n",
    "transform_train= transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=(-45,+45)),\n",
    "    transforms.ToTensor(),        \n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225])  # Normalization with the values for the pre-trained Resnet model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "29580af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the path to the training dataset\n",
    "data_train = Path(f\"{DATASET_DIR}/organized_data_inrae/train\")\n",
    "\n",
    "# Load dataset with ImageFolder\n",
    "train_dataset = ImageFolder(root=data_train, transform=transform_train)\n",
    "\n",
    "# Get class names\n",
    "class_names = train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8841b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Transformation pipeline without data augmentation for the validation and the test set\n",
    "\n",
    "transform= transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Redimensionnement √† 224x224\n",
    "    transforms.ToTensor(),          # Conversion en tenseur\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225])  # Normalisation avec les valeurs du mod√®le pr√©-entra√Æn√© Resnet\n",
    "])\n",
    "\n",
    "# Create the path to the training dataset\n",
    "data_val = Path(f\"{DATASET_DIR}/organized_data_inrae/val\")\n",
    "data_test = Path(f\"{DATASET_DIR}/organized_data_inrae/test\")\n",
    "\n",
    "# Load dataset with ImageFolder\n",
    "val_dataset = ImageFolder(root=data_val,transform=transform)\n",
    "test_dataset = ImageFolder(root=data_test, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "174b6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b7e983",
   "metadata": {},
   "source": [
    "## **Fine tuning (Resnet18)** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a677dfea",
   "metadata": {},
   "source": [
    "### Preparing the MLFlow tracking ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d9da841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tracking URI to your Hugging Face application\n",
    "MLFLOW_URI=os.getenv('MLFLOW_URI',\"https://gviel-mlflow37.hf.space/\")\n",
    "mlflow.set_tracking_uri(os.environ[\"MLFLOW_URI\"])\n",
    "\n",
    "# Set experiment's info\n",
    "EXPERIMENT_NAME= os.getenv('EXPERIMENT_NAME',\"Vitiscan_CNN_MLFlow_FineTuning\")+\"FINE_TUNING\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# Get our experiment info\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d57000",
   "metadata": {},
   "source": [
    "### Importing a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eb1666c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: mps:0\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "#model = models.resnet18(pretrained=True) # pour version <0.13.0\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT) # pour version >=0.13.0\n",
    "model = model.to(device)\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c762ac3b",
   "metadata": {},
   "source": [
    "### Replace classification layer to adapt the model to our features ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2ff3244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes=len(train_dataset.classes)\n",
    "model.fc = nn.Linear(model.fc.in_features, nb_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb65f752",
   "metadata": {},
   "source": [
    "### Freeze the feature extraction layers ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3c5d072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We freeze the entire network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# We unfreeze only the last Resnet18 blocks of layers (called \"Layer4 for resnet18\")\n",
    "# AND we unfreeze the classifier fc\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer4\" in name or \"fc\" in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dec27343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 7]                    --\n",
       "‚îú‚îÄConv2d: 1-1                            [1, 64, 112, 112]         (9,408)\n",
       "‚îú‚îÄBatchNorm2d: 1-2                       [1, 64, 112, 112]         (128)\n",
       "‚îú‚îÄReLU: 1-3                              [1, 64, 112, 112]         --\n",
       "‚îú‚îÄMaxPool2d: 1-4                         [1, 64, 56, 56]           --\n",
       "‚îú‚îÄSequential: 1-5                        [1, 64, 56, 56]           --\n",
       "‚îÇ    ‚îî‚îÄBasicBlock: 2-1                   [1, 64, 56, 56]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-1                  [1, 64, 56, 56]           (36,864)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-2             [1, 64, 56, 56]           (128)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-3                    [1, 64, 56, 56]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-4                  [1, 64, 56, 56]           (36,864)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-5             [1, 64, 56, 56]           (128)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-6                    [1, 64, 56, 56]           --\n",
       "‚îÇ    ‚îî‚îÄBasicBlock: 2-2                   [1, 64, 56, 56]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-7                  [1, 64, 56, 56]           (36,864)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-8             [1, 64, 56, 56]           (128)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-9                    [1, 64, 56, 56]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-10                 [1, 64, 56, 56]           (36,864)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-11            [1, 64, 56, 56]           (128)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-12                   [1, 64, 56, 56]           --\n",
       "‚îú‚îÄSequential: 1-6                        [1, 128, 28, 28]          --\n",
       "‚îÇ    ‚îî‚îÄBasicBlock: 2-3                   [1, 128, 28, 28]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-13                 [1, 128, 28, 28]          (73,728)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-14            [1, 128, 28, 28]          (256)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-15                   [1, 128, 28, 28]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-16                 [1, 128, 28, 28]          (147,456)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-17            [1, 128, 28, 28]          (256)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-18             [1, 128, 28, 28]          (8,448)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-19                   [1, 128, 28, 28]          --\n",
       "‚îÇ    ‚îî‚îÄBasicBlock: 2-4                   [1, 128, 28, 28]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-20                 [1, 128, 28, 28]          (147,456)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-21            [1, 128, 28, 28]          (256)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-22                   [1, 128, 28, 28]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-23                 [1, 128, 28, 28]          (147,456)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-24            [1, 128, 28, 28]          (256)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-25                   [1, 128, 28, 28]          --\n",
       "‚îú‚îÄSequential: 1-7                        [1, 256, 14, 14]          --\n",
       "‚îÇ    ‚îî‚îÄBasicBlock: 2-5                   [1, 256, 14, 14]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-26                 [1, 256, 14, 14]          (294,912)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-27            [1, 256, 14, 14]          (512)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-28                   [1, 256, 14, 14]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-29                 [1, 256, 14, 14]          (589,824)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-30            [1, 256, 14, 14]          (512)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-31             [1, 256, 14, 14]          (33,280)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-32                   [1, 256, 14, 14]          --\n",
       "‚îÇ    ‚îî‚îÄBasicBlock: 2-6                   [1, 256, 14, 14]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-33                 [1, 256, 14, 14]          (589,824)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-34            [1, 256, 14, 14]          (512)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-35                   [1, 256, 14, 14]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-36                 [1, 256, 14, 14]          (589,824)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-37            [1, 256, 14, 14]          (512)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-38                   [1, 256, 14, 14]          --\n",
       "‚îú‚îÄSequential: 1-8                        [1, 512, 7, 7]            --\n",
       "‚îÇ    ‚îî‚îÄBasicBlock: 2-7                   [1, 512, 7, 7]            --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-39                 [1, 512, 7, 7]            1,179,648\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-40            [1, 512, 7, 7]            1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-41                   [1, 512, 7, 7]            --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-42                 [1, 512, 7, 7]            2,359,296\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-43            [1, 512, 7, 7]            1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-44             [1, 512, 7, 7]            132,096\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-45                   [1, 512, 7, 7]            --\n",
       "‚îÇ    ‚îî‚îÄBasicBlock: 2-8                   [1, 512, 7, 7]            --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-46                 [1, 512, 7, 7]            2,359,296\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-47            [1, 512, 7, 7]            1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-48                   [1, 512, 7, 7]            --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-49                 [1, 512, 7, 7]            2,359,296\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-50            [1, 512, 7, 7]            1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-51                   [1, 512, 7, 7]            --\n",
       "‚îú‚îÄAdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --\n",
       "‚îú‚îÄLinear: 1-10                           [1, 7]                    3,591\n",
       "==========================================================================================\n",
       "Total params: 11,180,103\n",
       "Trainable params: 8,397,319\n",
       "Non-trainable params: 2,782,784\n",
       "Total mult-adds (Units.GIGABYTES): 1.81\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 39.74\n",
       "Params size (MB): 44.72\n",
       "Estimated Total Size (MB): 85.06\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print model summary\n",
    "summary(model, input_size=(1, 3, 224, 224))  # (batch_size, input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827dc7ed",
   "metadata": {},
   "source": [
    "### Defining the cost function and optimizer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c085ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device) \n",
    "\n",
    "learning_rate=0.0001\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=0.0001 # Ridge regulation to avoid overfitting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94632022",
   "metadata": {},
   "source": [
    "### Train the model ###\n",
    "\n",
    "We check an output on the fist batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "222aa307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.6696e-01,  1.1915e+00,  3.9915e-01, -5.1671e-01, -4.6692e-01,\n",
       "          1.4614e-01, -1.2787e+00],\n",
       "        [ 9.6687e-01,  1.0571e+00, -3.8468e-01, -6.1200e-01, -1.9921e-01,\n",
       "          3.9960e-01, -9.8709e-01],\n",
       "        [-6.0490e-02,  1.1982e+00,  5.6133e-01, -7.5117e-01, -5.2219e-01,\n",
       "         -3.8251e-01, -8.5225e-01],\n",
       "        [-2.0685e-01,  5.7663e-01,  3.3269e-01, -4.3178e-01, -5.3747e-01,\n",
       "          1.1735e-01, -1.8142e+00],\n",
       "        [ 2.3610e-01,  1.1594e+00, -4.9645e-01, -8.4795e-01, -6.9856e-01,\n",
       "         -1.3576e-01, -1.9910e+00],\n",
       "        [ 2.5394e-01,  4.2684e-01, -2.1686e-01, -8.3230e-01, -3.7474e-01,\n",
       "         -4.3500e-02, -1.3902e+00],\n",
       "        [-3.3378e-01,  1.0401e+00,  9.7580e-02, -3.1863e-01, -4.1905e-01,\n",
       "         -5.3801e-01, -7.7645e-01],\n",
       "        [ 5.2134e-01,  1.4445e+00,  7.2755e-02, -3.8107e-01, -4.5879e-01,\n",
       "         -5.6436e-01, -1.9546e+00],\n",
       "        [ 1.5829e-01,  6.7098e-01,  6.6299e-02, -7.2604e-01,  2.2335e-01,\n",
       "         -7.0277e-01, -1.2803e+00],\n",
       "        [ 7.5127e-01,  3.8958e-01,  5.1347e-01, -6.6653e-01, -3.6884e-01,\n",
       "         -3.2033e-01, -1.9184e+00],\n",
       "        [-4.5410e-01,  5.8055e-01, -1.6257e-01, -7.8936e-01, -7.3562e-01,\n",
       "         -3.5592e-01, -1.9152e+00],\n",
       "        [ 5.7503e-01,  1.0948e+00,  1.9598e-01, -3.4369e-01, -3.6001e-02,\n",
       "         -6.2769e-01, -1.2538e+00],\n",
       "        [ 3.7600e-02,  4.3516e-01,  3.5564e-01, -6.2971e-01, -3.4012e-01,\n",
       "         -8.5762e-01, -1.2345e+00],\n",
       "        [-2.1164e-01,  9.0657e-01, -7.7476e-01, -1.1838e+00, -4.9066e-01,\n",
       "         -6.7767e-01, -2.1516e+00],\n",
       "        [-9.5971e-02,  9.9944e-01,  1.3982e-01, -6.2724e-01,  1.3964e-02,\n",
       "         -5.2989e-01, -1.2107e+00],\n",
       "        [ 9.6343e-02,  1.3107e+00,  7.1204e-02, -9.4953e-01, -1.1322e+00,\n",
       "         -7.2458e-01, -1.9395e+00],\n",
       "        [ 2.4661e-03,  1.2395e+00,  7.9048e-01, -1.2625e+00, -2.5507e-01,\n",
       "         -6.9331e-01, -2.0429e+00],\n",
       "        [-1.1364e+00,  3.5774e-01,  1.8845e-01, -7.6697e-01, -2.5036e-01,\n",
       "         -6.4460e-05, -1.5019e+00],\n",
       "        [ 9.3636e-02,  9.9460e-01,  3.6752e-01, -5.6249e-01, -2.4483e-02,\n",
       "         -3.6980e-01, -1.0571e+00],\n",
       "        [ 9.5791e-01,  3.3191e-01,  5.1417e-01, -9.0377e-01, -3.7637e-01,\n",
       "         -3.9162e-01, -1.4567e+00],\n",
       "        [-3.7849e-02,  8.9436e-01,  4.4676e-01, -1.3839e+00, -4.9235e-01,\n",
       "         -8.4360e-01, -2.1900e+00],\n",
       "        [ 2.2768e-01,  1.4446e+00,  3.7678e-01, -5.4462e-01, -9.8586e-02,\n",
       "         -8.4047e-01, -1.3112e+00],\n",
       "        [ 1.4001e-02,  8.4662e-01,  6.3608e-01, -1.1992e+00,  3.1642e-01,\n",
       "          7.9495e-02, -1.4975e+00],\n",
       "        [ 8.0638e-02,  8.6155e-01,  6.9598e-01, -7.8557e-01, -1.1411e+00,\n",
       "         -3.3980e-01, -1.6357e+00],\n",
       "        [ 9.1369e-01,  1.1903e+00, -2.0466e-01, -3.1765e-01, -2.8689e-02,\n",
       "         -3.9430e-01, -9.6019e-01],\n",
       "        [-4.3085e-01,  7.6471e-01,  3.2387e-01, -1.4000e+00, -5.4496e-01,\n",
       "         -6.4873e-01, -1.5974e+00],\n",
       "        [-1.4304e-01,  7.7791e-01, -2.0417e-01, -6.0723e-01,  2.2464e-01,\n",
       "          1.4536e-02, -1.2997e+00],\n",
       "        [-4.4667e-01,  9.6894e-01,  2.7068e-01, -9.1882e-01, -6.0633e-01,\n",
       "         -3.1029e-01, -2.0485e+00],\n",
       "        [ 2.2461e-01,  1.0630e+00,  4.1985e-01, -8.6533e-01, -7.5761e-01,\n",
       "          4.5797e-01, -1.1317e+00],\n",
       "        [-8.1119e-01,  1.2309e+00,  3.0388e-01, -8.0152e-01, -2.4020e-01,\n",
       "         -1.0007e-01, -1.8363e+00],\n",
       "        [ 2.0909e-01,  1.2833e+00,  3.6020e-01, -1.3870e+00, -5.1195e-01,\n",
       "         -9.0912e-02, -1.7866e+00],\n",
       "        [ 7.1255e-01,  7.0189e-01, -2.2894e-01, -9.2624e-01, -5.2106e-01,\n",
       "         -5.1561e-01, -1.2184e+00]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model outputs\n",
    "image, label = next(iter(train_loader))\n",
    "image=image.to(device)\n",
    "model=model.to(device)\n",
    "logit= model(image) # Resnet18 output\n",
    "logit\n",
    "# Attention il faut que l'image et le modele aient le m√™me device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6900526c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['colomerus_vitis',\n",
       " 'elsinoe_ampelina',\n",
       " 'erysiphe_necator',\n",
       " 'guignardia_bidwellii',\n",
       " 'phaeomoniella_chlamydospora',\n",
       " 'plasmopara_viticola',\n",
       " 'sain']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0be446dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_test(model, test_loader, device):\n",
    "    \n",
    "    model.eval() # Mode √©valuation\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logit = model(images)\n",
    "            preds = logit.argmax(dim=1)\n",
    "            \n",
    "            # Compter l'exactitude\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            \n",
    "            # Collecter pour les m√©triques de Scikit-learn\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    \n",
    "    test_accuracy = correct / total\n",
    "    return test_accuracy, y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c354fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for a PyTorch model\n",
    "def train(model, train_loader, val_loader, test_loader, criterion, optimizer, experiment, epochs=20, patience=5):\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # We start a MLflow run\n",
    "    with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "\n",
    "        # Logging Pytorch parameters into MLflow \n",
    "        params = {\n",
    "            \"optimizer\": type(optimizer).__name__,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"epochs\": epochs,\n",
    "            \"criterion\": type(criterion).__name__,\n",
    "            \"model_architecture\": type(model).__name__,\n",
    "            \"training_device\": str(device),\n",
    "            \"weight_decay\": optimizer.param_groups[0][\"weight_decay\"]\n",
    "        }\n",
    "        \n",
    "        mlflow.log_params(params=params)\n",
    "        mlflow.pytorch.autolog()\n",
    "\n",
    "        # Dictionary to store loss and accuracy values for each epoch\n",
    "        history = {\n",
    "            'loss': [],\n",
    "            'val_loss': [],\n",
    "            'accuracy': [],\n",
    "            'val_accuracy': []\n",
    "        }\n",
    "\n",
    "        # ------------------- TRAINING LOOP -------------------\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_loss, correct = 0, 0  \n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logit = model(images)\n",
    "                loss = criterion(logit, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                correct += (logit.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "            train_loss = total_loss / len(train_loader)\n",
    "            train_acc = correct / len(train_loader.dataset)\n",
    "\n",
    "            # ------------------- VALIDATION LOOP -------------------\n",
    "            model.eval()\n",
    "            val_loss, val_correct = 0, 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    logit = model(images)\n",
    "                    loss = criterion(logit, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    val_correct += (logit.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "            # --- Save metrics ---\n",
    "            history['loss'].append(train_loss)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['accuracy'].append(train_acc)\n",
    "            history['val_accuracy'].append(val_acc)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{epochs}], \"\n",
    "                f\"Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, \"\n",
    "                f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "            #  Logging metrics\n",
    "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "            mlflow.log_metric(\"train_accuracy\", train_acc, step=epoch)\n",
    "            mlflow.log_metric(\"validation_loss\", val_loss, step=epoch)\n",
    "            mlflow.log_metric(\"validation_accuracy\", val_acc, step=epoch)\n",
    "\n",
    "            # --- Early stopping check ---\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = model.state_dict()  # save the best model\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                    model.load_state_dict(best_model_state)  # restore best model\n",
    "                    break\n",
    "                \n",
    "\n",
    "        # ================= FINAL VALIDATION EVALUATION =================\n",
    "        \n",
    "        print(\"\\n--- Final evaluation on the VALIDATION set ---\\n\")\n",
    "\n",
    "        y_true_val, y_pred_val = [], []\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                logit = model(images)\n",
    "                preds = logit.argmax(dim=1)\n",
    "\n",
    "                y_true_val.extend(labels.cpu().numpy())\n",
    "                y_pred_val.extend(preds.cpu().numpy())\n",
    "\n",
    "        # --- Precision & Recall (Validation) ---\n",
    "        precision_weighted_val = precision_score(\n",
    "            y_true_val, y_pred_val, average='weighted', zero_division=0\n",
    "        )\n",
    "        precision_macro_val = precision_score(\n",
    "            y_true_val, y_pred_val, average='macro', zero_division=0\n",
    "        )\n",
    "\n",
    "        recall_weighted_val = recall_score(\n",
    "            y_true_val, y_pred_val, average='weighted', zero_division=0\n",
    "        )\n",
    "        recall_macro_val = recall_score(\n",
    "            y_true_val, y_pred_val, average='macro', zero_division=0\n",
    "        )\n",
    "\n",
    "        mlflow.log_metric(\"Validation_precision_weighted\", precision_weighted_val)\n",
    "        mlflow.log_metric(\"Validation_precision_macro\", precision_macro_val)\n",
    "        mlflow.log_metric(\"Validation_recall_weighted\", recall_weighted_val)\n",
    "        mlflow.log_metric(\"Validation_recall_macro\", recall_macro_val)\n",
    "\n",
    "        # --- F1 Score (Validation) ---\n",
    "        f1_weighted_val = f1_score(y_true_val, y_pred_val, average='weighted')\n",
    "        f1_macro_val = f1_score(y_true_val, y_pred_val, average='macro')\n",
    "\n",
    "        mlflow.log_metric(\"Validation_f1_score_weighted\", f1_weighted_val)\n",
    "        mlflow.log_metric(\"Validation_f1_score_macro\", f1_macro_val)\n",
    "        mlflow.log_metric(\"final_validation_accuracy\", val_acc)\n",
    "\n",
    "        # Printing metrics\n",
    "        print(f\"Precision Validation (weighted): {precision_weighted_val:.4f}\")\n",
    "        print(f\"Recall Validation (weighted): {recall_weighted_val:.4f}\")\n",
    "        print(f\"F1 Score Validation (weighted): {f1_weighted_val:.4f}\")\n",
    "\n",
    "        # --- Confusion Matrix (Validation) ---\n",
    "        cm = confusion_matrix(y_true_val, y_pred_val)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(\n",
    "            cm, annot=True, fmt='d', cmap='Blues'\n",
    "        )\n",
    "\n",
    "        plt.xticks(ticks= range(nb_classes),labels=class_names, rotation=45, ha=\"right\")\n",
    "        plt.yticks(ticks= range(nb_classes),labels=class_names,rotation=0)\n",
    "        plt.ylabel('True class')\n",
    "        plt.xlabel('Predicted class')\n",
    "        plt.title('Confusion matrix - VALIDATION')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix_VALIDATION.png', dpi=150)\n",
    "\n",
    "        mlflow.log_artifact('confusion_matrix_VALIDATION.png')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        # ================= FINAL TEST EVALUATION =================\n",
    "        \n",
    "        print(\"\\n--- Final evaluation on the TEST set ---\\n\")\n",
    "\n",
    "        test_acc, y_true_test, y_pred_test = evaluate_model_on_test(\n",
    "            model, test_loader, device\n",
    "        )\n",
    "\n",
    "        # --- Precision & Recall (Test) ---\n",
    "        precision_weighted_test = precision_score(\n",
    "            y_true_test, y_pred_test, average='weighted', zero_division=0\n",
    "        )\n",
    "        precision_macro_test = precision_score(\n",
    "            y_true_test, y_pred_test, average='macro', zero_division=0\n",
    "        )\n",
    "\n",
    "        recall_weighted_test = recall_score(\n",
    "            y_true_test, y_pred_test, average='weighted', zero_division=0\n",
    "        )\n",
    "        recall_macro_test = recall_score(\n",
    "            y_true_test, y_pred_test, average='macro', zero_division=0\n",
    "        )\n",
    "\n",
    "        mlflow.log_metric(\"Test_precision_weighted\", precision_weighted_test)\n",
    "        mlflow.log_metric(\"Test_precision_macro\", precision_macro_test)\n",
    "        mlflow.log_metric(\"Test_recall_weighted\", recall_weighted_test)\n",
    "        mlflow.log_metric(\"Test_recall_macro\", recall_macro_test)\n",
    "\n",
    "        # --- F1 Score (Test) ---\n",
    "        f1_weighted_test = f1_score(y_true_test, y_pred_test, average='weighted')\n",
    "        f1_macro_test = f1_score(y_true_test, y_pred_test, average='macro')\n",
    "\n",
    "        mlflow.log_metric(\"Test_accuracy\", test_acc)\n",
    "        mlflow.log_metric(\"Test_f1score_weighted\", f1_weighted_test)\n",
    "        mlflow.log_metric(\"Test_f1score_macro\", f1_macro_test)\n",
    "\n",
    "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"Precision Test (weighted): {precision_weighted_test:.4f}\")\n",
    "        print(f\"Recall Test (weighted): {recall_weighted_test:.4f}\")\n",
    "        print(f\"F1 Score Test (weighted): {f1_weighted_test:.4f}\")\n",
    "\n",
    "        # --- Confusion Matrix (Test) ---\n",
    "        cm_test = confusion_matrix(y_true_test, y_pred_test)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(\n",
    "            cm_test, annot=True, fmt='d', cmap='Blues',\n",
    "        )\n",
    "        plt.xticks(ticks= range(nb_classes),labels=class_names, rotation=45, ha=\"right\")\n",
    "        plt.yticks(ticks= range(nb_classes),labels=class_names,rotation=0)\n",
    "\n",
    "        plt.ylabel('True class')\n",
    "        plt.xlabel('Predicted class')\n",
    "        plt.title('Confusion matrix - TEST')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix_TEST.png', dpi=150)\n",
    "\n",
    "        mlflow.log_artifact('confusion_matrix_TEST.png')\n",
    "        plt.close()\n",
    "\n",
    "        # --- Model logging ---\n",
    "        mlflow.pytorch.log_model(\n",
    "            pytorch_model=model,\n",
    "            artifact_path=\"Resnet18\",\n",
    "            registered_model_name=f\"{params['model_architecture']}\"\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- Metrics and model logged into MLflow ---\\n\")\n",
    "\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6bdd528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISEASES = {\n",
    "    \"colomerus_vitis\" : \"erinose\",\n",
    "    \"elsinoe_ampelina\" : \"anthracnose\",\n",
    "    \"erysiphe_necator\":\"oidium\",\n",
    "    \"guignardia_bidwellii\" : \"pourriture_noire\",\n",
    "    \"phaeomoniella_chlamydospora\" : \"esca\",\n",
    "    \"plasmopara_viticola\":\"mildiou\",\n",
    "    \"sain\" : \"sain\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "acfe273d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Loss: 0.0786, Acc: 0.9764, Val Loss: 0.2521, Val Acc: 0.9333\n",
      "Epoch [2/40], Loss: 0.0363, Acc: 0.9915, Val Loss: 0.2337, Val Acc: 0.9422\n",
      "Epoch [3/40], Loss: 0.0531, Acc: 0.9915, Val Loss: 0.2415, Val Acc: 0.9333\n",
      "Epoch [4/40], Loss: 0.0440, Acc: 0.9868, Val Loss: 0.2470, Val Acc: 0.9156\n",
      "Epoch [5/40], Loss: 0.0482, Acc: 0.9906, Val Loss: 0.2234, Val Acc: 0.9422\n",
      "Epoch [6/40], Loss: 0.0516, Acc: 0.9906, Val Loss: 0.2114, Val Acc: 0.9333\n",
      "Epoch [7/40], Loss: 0.0404, Acc: 0.9915, Val Loss: 0.2430, Val Acc: 0.9378\n",
      "Epoch [8/40], Loss: 0.0228, Acc: 0.9934, Val Loss: 0.2340, Val Acc: 0.9467\n",
      "Epoch [9/40], Loss: 0.0193, Acc: 0.9962, Val Loss: 0.2222, Val Acc: 0.9378\n",
      "Epoch [10/40], Loss: 0.0191, Acc: 0.9953, Val Loss: 0.2583, Val Acc: 0.9467\n",
      "Epoch [11/40], Loss: 0.0724, Acc: 0.9953, Val Loss: 0.2281, Val Acc: 0.9333\n",
      "Early stopping triggered after 11 epochs\n",
      "\n",
      "--- Final evaluation on the VALIDATION set ---\n",
      "\n",
      "Precision Validation (weighted): 0.9331\n",
      "Recall Validation (weighted): 0.9333\n",
      "F1 Score Validation (weighted): 0.9327\n",
      "\n",
      "--- Final evaluation on the TEST set ---\n",
      "\n",
      "Test Accuracy: 0.9191\n",
      "Precision Test (weighted): 0.9277\n",
      "Recall Test (weighted): 0.9191\n",
      "F1 Score Test (weighted): 0.9204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/19 15:12:46 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "Registered model 'ResNet' already exists. Creating a new version of this model...\n",
      "2025/12/19 15:13:41 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ResNet, version 22\n",
      "Created version '22' of model 'ResNet'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Metrics and model logged into MLflow ---\n",
      "\n",
      "üèÉ View run clean-goose-48 at: https://gviel-mlflow37.hf.space/#/experiments/4/runs/366fd4b021f440aeb5db947fa7717d4e\n",
      "üß™ View experiment at: https://gviel-mlflow37.hf.space/#/experiments/4\n"
     ]
    }
   ],
   "source": [
    "# Train the model and store the training history\n",
    "history = train(model, train_loader, val_loader, test_loader, criterion, optimizer, experiment, epochs=40,patience=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe11d29",
   "metadata": {},
   "source": [
    "### Visualization of the learning process ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49551d62",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from plotly import graph_objects as go\n",
    "color_chart = [\"#4B9AC7\", \"#4BE8E0\", \"#9DD4F3\", \"#97FBF6\", \"#2A7FAF\", \"#23B1AB\", \"#0E3449\", \"#015955\"]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "                      go.Scatter(\n",
    "                          y=history[\"loss\"],\n",
    "                          name=\"Training loss\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[0]\n",
    "                          )),\n",
    "                      go.Scatter(\n",
    "                          y=history[\"val_loss\"],\n",
    "                          name=\"Validation loss\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[1]\n",
    "                          ))\n",
    "])\n",
    "fig.update_layout(\n",
    "    title='Training and val loss across epochs',\n",
    "    xaxis_title='epochs',\n",
    "    yaxis_title='Cross Entropy'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bee8a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "color_chart = [\"#4B9AC7\", \"#4BE8E0\", \"#9DD4F3\", \"#97FBF6\", \"#2A7FAF\", \"#23B1AB\", \"#0E3449\", \"#015955\"]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "                      go.Scatter(\n",
    "                          y=history[\"accuracy\"],\n",
    "                          name=\"Training Accuracy\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[0]\n",
    "                          )),\n",
    "                      go.Scatter(\n",
    "                          y=history[\"val_accuracy\"],\n",
    "                          name=\"Validation Accuracy\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[1]\n",
    "                          ))\n",
    "])\n",
    "fig.update_layout(\n",
    "    title='Training and val Accuracy across epochs',\n",
    "    xaxis_title='epochs',\n",
    "    yaxis_title='Cross Entropy'\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vitiscan_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
