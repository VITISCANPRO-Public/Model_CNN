{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec7fb97b",
   "metadata": {},
   "source": [
    "# **--- CNN MODEL ---** #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d3b0f5",
   "metadata": {},
   "source": [
    "## **Library import** ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e19ebda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b04db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torchinfo 1.8.0\n",
      "Uninstalling torchinfo-1.8.0:\n",
      "  Would remove:\n",
      "    /opt/anaconda3/envs/vitiscan_cnn/lib/python3.11/site-packages/torchinfo-1.8.0.dist-info/*\n",
      "    /opt/anaconda3/envs/vitiscan_cnn/lib/python3.11/site-packages/torchinfo/*\n",
      "Proceed (Y/n)? "
     ]
    }
   ],
   "source": [
    "!pip uninstall torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a101012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from torchinfo import summary\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():  # Apple M1/M2/M3\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a30955",
   "metadata": {},
   "source": [
    "## **Image import** ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc70e17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"../data/train/train/Grape Anthracnose leaf/1.jpg\")\n",
    "#img.show()\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45dedc7",
   "metadata": {},
   "source": [
    "## **Image processing** ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e4a4c",
   "metadata": {},
   "source": [
    "### Pipeline for data transformation ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed0ed4d",
   "metadata": {},
   "source": [
    "The dataset already contains pictures that have been flipped horizontaly, verticaly and randomly rotated. We need to resize the pictures to the size required by the Resnet Model (224,224)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c1385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Redimensionnement à 224x224\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
    "    transforms.ToTensor(),          # Conversion en tenseur\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalisation avec les valeurs du modèle pré-entraîné\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ffdbe",
   "metadata": {},
   "source": [
    "### Load dataset ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3032e730",
   "metadata": {},
   "source": [
    "Our dataset is already divided into a train and a test set. We are going to extract 20% of the training set to create the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29580af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the path to the training dataset\n",
    "data_root = Path(\"../data/train\")\n",
    "\n",
    "# Load dataset with ImageFolder\n",
    "dataset = ImageFolder(root=data_root, transform=transform)\n",
    "\n",
    "# Get class names\n",
    "class_names = dataset.classes\n",
    "\n",
    "# Define the set size\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset)-train_size\n",
    "\n",
    "# We split our initial train dataset into the final train dataset (80%) and the validation dataset (20%)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a269a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"- Train dataset size : {train_size} \\n- Validation dataset size :  {val_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c649050",
   "metadata": {},
   "source": [
    "###---------------------------------\n",
    "Si on a le temps :  on peut faire une classe maladie avec toutes les maladies regroupées pour faire un test sain/malade (dans ce cas là on pourra faire une classe personnalisée avec Pytorch dataset et créer le dataset en utilisant cette classe)\n",
    "---------------------------------------###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b2710",
   "metadata": {},
   "source": [
    "Visualisation of a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3856bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "imgs, labels = next(iter(train_loader))\n",
    "for i, (img, label) in enumerate(zip(imgs, labels)):\n",
    "  true_label_name = class_names[label]\n",
    "  print(\"Label :\", true_label_name)\n",
    "  plt.imshow(img.permute(1, 2, 0).numpy())\n",
    "  plt.show()\n",
    "  if i==9: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b7e983",
   "metadata": {},
   "source": [
    "## **Transfert learning** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d57000",
   "metadata": {},
   "source": [
    "### Importing a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1666c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/vitiscan_cnn/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/vitiscan_cnn/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # freeze conv layers\n",
    "\n",
    "# Replace classifier\n",
    "nb_classes = len(train_dataset.dataset.classes) #train_dataset.dataset because we have a subset\n",
    "model.fc = nn.Linear(model.fc.in_features, nb_classes)\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3acc8d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ec545e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 1]                    --\n",
       "├─Conv2d: 1-1                            [1, 64, 112, 112]         (9,408)\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 112, 112]         (128)\n",
       "├─ReLU: 1-3                              [1, 64, 112, 112]         --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 56, 56]           --\n",
       "├─Sequential: 1-5                        [1, 64, 56, 56]           --\n",
       "│    └─BasicBlock: 2-1                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 56, 56]           (36,864)\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 56, 56]           (128)\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 56, 56]           (36,864)\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 56, 56]           (128)\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 56, 56]           --\n",
       "│    └─BasicBlock: 2-2                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-7                  [1, 64, 56, 56]           (36,864)\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 64, 56, 56]           (128)\n",
       "│    │    └─ReLU: 3-9                    [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-10                 [1, 64, 56, 56]           (36,864)\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 56, 56]           (128)\n",
       "│    │    └─ReLU: 3-12                   [1, 64, 56, 56]           --\n",
       "├─Sequential: 1-6                        [1, 128, 28, 28]          --\n",
       "│    └─BasicBlock: 2-3                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-13                 [1, 128, 28, 28]          (73,728)\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 128, 28, 28]          (256)\n",
       "│    │    └─ReLU: 3-15                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-16                 [1, 128, 28, 28]          (147,456)\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 128, 28, 28]          (256)\n",
       "│    │    └─Sequential: 3-18             [1, 128, 28, 28]          (8,448)\n",
       "│    │    └─ReLU: 3-19                   [1, 128, 28, 28]          --\n",
       "│    └─BasicBlock: 2-4                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-20                 [1, 128, 28, 28]          (147,456)\n",
       "│    │    └─BatchNorm2d: 3-21            [1, 128, 28, 28]          (256)\n",
       "│    │    └─ReLU: 3-22                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-23                 [1, 128, 28, 28]          (147,456)\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 128, 28, 28]          (256)\n",
       "│    │    └─ReLU: 3-25                   [1, 128, 28, 28]          --\n",
       "├─Sequential: 1-7                        [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-5                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-26                 [1, 256, 14, 14]          (294,912)\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-28                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 256, 14, 14]          (512)\n",
       "│    │    └─Sequential: 3-31             [1, 256, 14, 14]          (33,280)\n",
       "│    │    └─ReLU: 3-32                   [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-6                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-33                 [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-34            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-35                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-36                 [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-37            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-38                   [1, 256, 14, 14]          --\n",
       "├─Sequential: 1-8                        [1, 512, 7, 7]            --\n",
       "│    └─BasicBlock: 2-7                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-39                 [1, 512, 7, 7]            (1,179,648)\n",
       "│    │    └─BatchNorm2d: 3-40            [1, 512, 7, 7]            (1,024)\n",
       "│    │    └─ReLU: 3-41                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-42                 [1, 512, 7, 7]            (2,359,296)\n",
       "│    │    └─BatchNorm2d: 3-43            [1, 512, 7, 7]            (1,024)\n",
       "│    │    └─Sequential: 3-44             [1, 512, 7, 7]            (132,096)\n",
       "│    │    └─ReLU: 3-45                   [1, 512, 7, 7]            --\n",
       "│    └─BasicBlock: 2-8                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-46                 [1, 512, 7, 7]            (2,359,296)\n",
       "│    │    └─BatchNorm2d: 3-47            [1, 512, 7, 7]            (1,024)\n",
       "│    │    └─ReLU: 3-48                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-49                 [1, 512, 7, 7]            (2,359,296)\n",
       "│    │    └─BatchNorm2d: 3-50            [1, 512, 7, 7]            (1,024)\n",
       "│    │    └─ReLU: 3-51                   [1, 512, 7, 7]            --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --\n",
       "├─Linear: 1-10                           [1, 1]                    513\n",
       "==========================================================================================\n",
       "Total params: 11,177,025\n",
       "Trainable params: 513\n",
       "Non-trainable params: 11,176,512\n",
       "Total mult-adds (Units.GIGABYTES): 1.81\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 39.74\n",
       "Params size (MB): 44.71\n",
       "Estimated Total Size (MB): 85.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print model summary\n",
    "summary(model, input_size=(1, 3, 224, 224))  # (batch_size, input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec60433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the feature extraction layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58408bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058ada3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modify the final classification layer to match the number of classes\n",
    "num_classes = len(train_dataset.dataset.dataset.classes)\n",
    "\n",
    "num_classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vitiscan_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
